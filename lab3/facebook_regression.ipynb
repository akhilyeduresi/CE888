{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have loaded the necessary libraries above\n",
    "* Now let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset_Facebook.csv\", delimiter = \";\")\n",
    "df\n",
    "#print(\"The dataset has %d columns and %d rows\" % (df.shape[1], df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Category\",\n",
    "            \"Page total likes\",\n",
    "            \"Type\",\n",
    "            \"Post Month\",\n",
    "            \"Post Hour\",\n",
    "            \"Post Weekday\",\n",
    "            \"Paid\"]\n",
    "\n",
    "\n",
    "df[features].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes=  [\"Lifetime Post Total Reach\",\n",
    "            \"Lifetime Post Total Impressions\",\n",
    "            \"Lifetime Engaged Users\",\n",
    "            \"Lifetime Post Consumers\",\n",
    "            \"Lifetime Post Consumptions\",\n",
    "            \"Lifetime Post Impressions by people who have liked your Page\",\n",
    "            \"Lifetime Post reach by people who like your Page\",\n",
    "            \"Lifetime People who have liked your Page and engaged with your post\",\n",
    "            \"comment\",\n",
    "            \"like\",\n",
    "            \"share\",\n",
    "            \"Total Interactions\"]\n",
    "\n",
    "df[outcomes].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert a string variable to a categorical one\n",
    "#types = list(set(df[\"Type\"]))\n",
    "#to_categorical = {types[i]:i for i in range(len(types))}\n",
    "#df[\"Type\"] = df[\"Type\"].apply(lambda x: to_categorical[x])\n",
    "\n",
    "df[[\"Type\"]] = df[[\"Type\"]].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare the data by cleaning it up and choosing the relevant column we would like to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]\n",
    "\n",
    "outcomes_of_interest = [\"Lifetime Post Consumers\", \"like\"]\n",
    "\n",
    "X_df = df[features].copy()\n",
    "y_df = df[outcomes_of_interest].copy()\n",
    "\n",
    "cat_features = [\"Category\", \"Type\", \"Paid\"]\n",
    "#print(X_df.head())\n",
    "#print(set(X_df[\"Category\"]))\n",
    "#print(pd.get_dummies(X_df, columns=[\"Category\"]).head())\n",
    "\n",
    "X_df = pd.get_dummies(X_df, columns=cat_features)\n",
    "\n",
    "print(X_df.head()[[\"Category_1\", \"Category_2\", \"Category_3\"]].to_latex())\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values.T[0]\n",
    "\n",
    "y = (y-y.min())/(y.max() - y.min())\n",
    "\n",
    "y_df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "#y_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.lmplot(x=\"id\", y=\"Lifetime Post Consumers\", data=y_df, fit_reg=False, aspect = 2)\n",
    "\n",
    "# sns_plot.savefig(\"scaterplot_lpc.png\",bbox_inches='tight')\n",
    "# sns_plot.savefig(\"scaterplot_lpc.pdf\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.jointplot(x=\"Lifetime Post Consumers\", y=\"like\", data=y_df, ratio = 2)\n",
    "\n",
    "# sns_plot.savefig(\"joint_plot.png\",bbox_inches='tight')\n",
    "# sns_plot.savefig(\"joint_plot.pdf\",bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.distplot(y, kde=False, rug=True)\n",
    "\n",
    "# sns_plot.savefig(\"histogram_lpc.png\",bbox_inches='tight')\n",
    "# sns_plot.savefig(\"histogram_lpc.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a classifier and then use the bootstrap to find an approximation of the bias and the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 100\n",
    "n_repeat = 1000\n",
    "\n",
    "#estimator = DecisionTreeRegressor()\n",
    "estimator = RandomForestRegressor()\n",
    "#estimator = BayesianRidge(normalize = True)\n",
    "\n",
    "\n",
    "# Compute predictions\n",
    "y_predicts = np.zeros((n_repeat, len(X)))\n",
    "#stdy = y/y.max()\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    \n",
    "    sample  = np.random.choice(range(len(X)),replace = True, size = len(X))\n",
    "    \n",
    "    train_ids = sample[:-n_test]\n",
    "    test_ids  = sample[-n_test:]\n",
    "    test_ids = np.setdiff1d(test_ids, train_ids)\n",
    "    if(len(test_ids) == 0 ):\n",
    "        continue\n",
    "\n",
    "    X_train,y_train = X[train_ids], y[train_ids]\n",
    "    X_test, y_test = X[test_ids], y[test_ids]\n",
    "    \n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    y_predicts[i,test_ids] = y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bias = (y - np.mean(y_predicts, axis=0)) **2\n",
    "y_error = ((y - y_predicts) **2).mean()\n",
    "y_var = np.var(y_predicts, axis=0, ddof = 1)\n",
    "\n",
    "\n",
    "print(np.mean(y_bias) +  np.mean(y_var))\n",
    "\n",
    "clf_type = \"Decision tree\"\n",
    "print(\"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n",
    "          \"+ {3:.4f} (var)\".format(clf_type, np.mean(y_error), np.mean(y_bias), np.mean(y_var)))\n",
    "\n",
    "print(\"{0}: {1:.4f} ((bias^2) + (var)) = {2:.4f} (bias^2) \"\n",
    "          \"+ {3:.4f} (var)\".format(clf_type, np.mean(y_bias) + np.mean(y_var), np.mean(y_bias), np.mean(y_var)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators = 1000,max_depth = 2)\n",
    "from sklearn.linear_model import SGDRegressor, BayesianRidge\n",
    "\n",
    "clf = BayesianRidge(normalize = True)\n",
    "\n",
    "\n",
    "dummy_clf = DummyRegressor()\n",
    "scores = cross_val_score(clf, X, y, cv=10,scoring = make_scorer(mse))\n",
    "dummy_scores = cross_val_score(dummy_clf, X, y, cv=10, scoring = make_scorer(mse))\n",
    "\n",
    "print(\"MSE: %0.8f (+/- %0.8f)\" % (scores.mean(), scores.std()))\n",
    "print(\"Dummy MSE: %0.8f (+/- %0.8f)\" % (dummy_scores.mean(), dummy_scores.std()))\n",
    "\n",
    "#print clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the regressor on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import BaggingRegressor\n",
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#clf = RandomForestRegressor(n_estimators = 500, criterion = \"mse\")\n",
    "#clf = DecisionTreeRegressor()\n",
    "clf = BayesianRidge()\n",
    "#clf = AdaBoostRegressor(DecisionTreeRegressor(),n_estimators = 1000)#\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "stdy = y\n",
    "clf.fit(X,stdy)\n",
    "\n",
    "print(mse(stdy,clf.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
