{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "#loading libraries\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential \n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Loading train data\n",
    "train=pd.read_csv(\"../input/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ee8cb7933996db2cc7e1893d0c812bb0d3d2fb15"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_2</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>InsuredInfo_1</th>\n",
       "      <th>InsuredInfo_2</th>\n",
       "      <th>InsuredInfo_3</th>\n",
       "      <th>InsuredInfo_4</th>\n",
       "      <th>InsuredInfo_5</th>\n",
       "      <th>InsuredInfo_6</th>\n",
       "      <th>InsuredInfo_7</th>\n",
       "      <th>Insurance_History_1</th>\n",
       "      <th>Insurance_History_2</th>\n",
       "      <th>Insurance_History_3</th>\n",
       "      <th>Insurance_History_4</th>\n",
       "      <th>Insurance_History_5</th>\n",
       "      <th>Insurance_History_7</th>\n",
       "      <th>Insurance_History_8</th>\n",
       "      <th>Insurance_History_9</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>Family_Hist_4</th>\n",
       "      <th>Family_Hist_5</th>\n",
       "      <th>Medical_History_1</th>\n",
       "      <th>Medical_History_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_10</th>\n",
       "      <th>Medical_Keyword_11</th>\n",
       "      <th>Medical_Keyword_12</th>\n",
       "      <th>Medical_Keyword_13</th>\n",
       "      <th>Medical_Keyword_14</th>\n",
       "      <th>Medical_Keyword_15</th>\n",
       "      <th>Medical_Keyword_16</th>\n",
       "      <th>Medical_Keyword_17</th>\n",
       "      <th>Medical_Keyword_18</th>\n",
       "      <th>Medical_Keyword_19</th>\n",
       "      <th>Medical_Keyword_20</th>\n",
       "      <th>Medical_Keyword_21</th>\n",
       "      <th>Medical_Keyword_22</th>\n",
       "      <th>Medical_Keyword_23</th>\n",
       "      <th>Medical_Keyword_24</th>\n",
       "      <th>Medical_Keyword_25</th>\n",
       "      <th>Medical_Keyword_26</th>\n",
       "      <th>Medical_Keyword_27</th>\n",
       "      <th>Medical_Keyword_28</th>\n",
       "      <th>Medical_Keyword_29</th>\n",
       "      <th>Medical_Keyword_30</th>\n",
       "      <th>Medical_Keyword_31</th>\n",
       "      <th>Medical_Keyword_32</th>\n",
       "      <th>Medical_Keyword_33</th>\n",
       "      <th>Medical_Keyword_34</th>\n",
       "      <th>Medical_Keyword_35</th>\n",
       "      <th>Medical_Keyword_36</th>\n",
       "      <th>Medical_Keyword_37</th>\n",
       "      <th>Medical_Keyword_38</th>\n",
       "      <th>Medical_Keyword_39</th>\n",
       "      <th>Medical_Keyword_40</th>\n",
       "      <th>Medical_Keyword_41</th>\n",
       "      <th>Medical_Keyword_42</th>\n",
       "      <th>Medical_Keyword_43</th>\n",
       "      <th>Medical_Keyword_44</th>\n",
       "      <th>Medical_Keyword_45</th>\n",
       "      <th>Medical_Keyword_46</th>\n",
       "      <th>Medical_Keyword_47</th>\n",
       "      <th>Medical_Keyword_48</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>D3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.148536</td>\n",
       "      <td>0.323008</td>\n",
       "      <td>0.028</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.526786</td>\n",
       "      <td>4.0</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.131799</td>\n",
       "      <td>0.272288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>412</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>E1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.288703</td>\n",
       "      <td>0.428780</td>\n",
       "      <td>0.030</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>D4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.205021</td>\n",
       "      <td>0.352438</td>\n",
       "      <td>0.042</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>D2</td>\n",
       "      <td>26</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.234310</td>\n",
       "      <td>0.424046</td>\n",
       "      <td>0.027</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Product_Info_1    ...    Medical_Keyword_48  Response\n",
       "0   2               1    ...                     0         8\n",
       "1   5               1    ...                     0         4\n",
       "2   6               1    ...                     0         8\n",
       "3   7               1    ...                     0         8\n",
       "4   8               1    ...                     0         8\n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59381, 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "73f01cc7a52560d940a79fbe7611ad16868db99f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Employment_Info_1         19\n",
       "Employment_Info_4       6779\n",
       "Employment_Info_6      10854\n",
       "Insurance_History_5    25396\n",
       "Family_Hist_2          28656\n",
       "Family_Hist_3          34241\n",
       "Family_Hist_4          19184\n",
       "Family_Hist_5          41811\n",
       "Medical_History_1       8889\n",
       "Medical_History_10     58824\n",
       "Medical_History_15     44596\n",
       "Medical_History_24     55580\n",
       "Medical_History_32     58274\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exploring missing values\n",
    "train.isnull().sum()[train.isnull().sum() !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "894fe7bff6368117131409e43a6263f61f38538a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_count_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employment_Info_1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.031997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employment_Info_4</td>\n",
       "      <td>6779</td>\n",
       "      <td>11.416110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Employment_Info_6</td>\n",
       "      <td>10854</td>\n",
       "      <td>18.278574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insurance_History_5</td>\n",
       "      <td>25396</td>\n",
       "      <td>42.767889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Family_Hist_2</td>\n",
       "      <td>28656</td>\n",
       "      <td>48.257860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Family_Hist_3</td>\n",
       "      <td>34241</td>\n",
       "      <td>57.663226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Family_Hist_4</td>\n",
       "      <td>19184</td>\n",
       "      <td>32.306630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Family_Hist_5</td>\n",
       "      <td>41811</td>\n",
       "      <td>70.411411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Medical_History_1</td>\n",
       "      <td>8889</td>\n",
       "      <td>14.969435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Medical_History_10</td>\n",
       "      <td>58824</td>\n",
       "      <td>99.061990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Medical_History_15</td>\n",
       "      <td>44596</td>\n",
       "      <td>75.101463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Medical_History_24</td>\n",
       "      <td>55580</td>\n",
       "      <td>93.598963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Medical_History_32</td>\n",
       "      <td>58274</td>\n",
       "      <td>98.135767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               features            ...             missing_count_percentage\n",
       "0     Employment_Info_1            ...                             0.031997\n",
       "1     Employment_Info_4            ...                            11.416110\n",
       "2     Employment_Info_6            ...                            18.278574\n",
       "3   Insurance_History_5            ...                            42.767889\n",
       "4         Family_Hist_2            ...                            48.257860\n",
       "5         Family_Hist_3            ...                            57.663226\n",
       "6         Family_Hist_4            ...                            32.306630\n",
       "7         Family_Hist_5            ...                            70.411411\n",
       "8     Medical_History_1            ...                            14.969435\n",
       "9    Medical_History_10            ...                            99.061990\n",
       "10   Medical_History_15            ...                            75.101463\n",
       "11   Medical_History_24            ...                            93.598963\n",
       "12   Medical_History_32            ...                            98.135767\n",
       "\n",
       "[13 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Exploring missing values\n",
    "train_missing= train.isnull().sum()[train.isnull().sum() !=0]\n",
    "train_missing=pd.DataFrame(train_missing.reset_index())\n",
    "train_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\n",
    "train_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(y=train_missing['features'],x=train_missing['missing_count_percentage'])\n",
    "train_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64'), dtype('O'), dtype('float64')], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking data types\n",
    "train.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8f7eea1d4dc8b50b49c05e009283dccfdb7c1b98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>InsuredInfo_1</th>\n",
       "      <th>InsuredInfo_2</th>\n",
       "      <th>InsuredInfo_3</th>\n",
       "      <th>InsuredInfo_4</th>\n",
       "      <th>InsuredInfo_5</th>\n",
       "      <th>InsuredInfo_6</th>\n",
       "      <th>InsuredInfo_7</th>\n",
       "      <th>Insurance_History_1</th>\n",
       "      <th>Insurance_History_2</th>\n",
       "      <th>Insurance_History_3</th>\n",
       "      <th>Insurance_History_4</th>\n",
       "      <th>Insurance_History_5</th>\n",
       "      <th>Insurance_History_7</th>\n",
       "      <th>Insurance_History_8</th>\n",
       "      <th>Insurance_History_9</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>Family_Hist_4</th>\n",
       "      <th>Family_Hist_5</th>\n",
       "      <th>Medical_History_1</th>\n",
       "      <th>Medical_History_2</th>\n",
       "      <th>Medical_History_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_10</th>\n",
       "      <th>Medical_Keyword_11</th>\n",
       "      <th>Medical_Keyword_12</th>\n",
       "      <th>Medical_Keyword_13</th>\n",
       "      <th>Medical_Keyword_14</th>\n",
       "      <th>Medical_Keyword_15</th>\n",
       "      <th>Medical_Keyword_16</th>\n",
       "      <th>Medical_Keyword_17</th>\n",
       "      <th>Medical_Keyword_18</th>\n",
       "      <th>Medical_Keyword_19</th>\n",
       "      <th>Medical_Keyword_20</th>\n",
       "      <th>Medical_Keyword_21</th>\n",
       "      <th>Medical_Keyword_22</th>\n",
       "      <th>Medical_Keyword_23</th>\n",
       "      <th>Medical_Keyword_24</th>\n",
       "      <th>Medical_Keyword_25</th>\n",
       "      <th>Medical_Keyword_26</th>\n",
       "      <th>Medical_Keyword_27</th>\n",
       "      <th>Medical_Keyword_28</th>\n",
       "      <th>Medical_Keyword_29</th>\n",
       "      <th>Medical_Keyword_30</th>\n",
       "      <th>Medical_Keyword_31</th>\n",
       "      <th>Medical_Keyword_32</th>\n",
       "      <th>Medical_Keyword_33</th>\n",
       "      <th>Medical_Keyword_34</th>\n",
       "      <th>Medical_Keyword_35</th>\n",
       "      <th>Medical_Keyword_36</th>\n",
       "      <th>Medical_Keyword_37</th>\n",
       "      <th>Medical_Keyword_38</th>\n",
       "      <th>Medical_Keyword_39</th>\n",
       "      <th>Medical_Keyword_40</th>\n",
       "      <th>Medical_Keyword_41</th>\n",
       "      <th>Medical_Keyword_42</th>\n",
       "      <th>Medical_Keyword_43</th>\n",
       "      <th>Medical_Keyword_44</th>\n",
       "      <th>Medical_Keyword_45</th>\n",
       "      <th>Medical_Keyword_46</th>\n",
       "      <th>Medical_Keyword_47</th>\n",
       "      <th>Medical_Keyword_48</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59362.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>52602.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>48527.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>33985.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>30725.000000</td>\n",
       "      <td>25140.000000</td>\n",
       "      <td>40197.000000</td>\n",
       "      <td>17570.000000</td>\n",
       "      <td>50492.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "      <td>59381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39507.211515</td>\n",
       "      <td>1.026355</td>\n",
       "      <td>24.415655</td>\n",
       "      <td>0.328952</td>\n",
       "      <td>2.006955</td>\n",
       "      <td>2.673599</td>\n",
       "      <td>1.043583</td>\n",
       "      <td>0.405567</td>\n",
       "      <td>0.707283</td>\n",
       "      <td>0.292587</td>\n",
       "      <td>0.469462</td>\n",
       "      <td>0.077582</td>\n",
       "      <td>8.641821</td>\n",
       "      <td>1.300904</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>2.142958</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>1.209326</td>\n",
       "      <td>2.007427</td>\n",
       "      <td>5.835840</td>\n",
       "      <td>2.883666</td>\n",
       "      <td>1.027180</td>\n",
       "      <td>1.409188</td>\n",
       "      <td>1.038531</td>\n",
       "      <td>1.727606</td>\n",
       "      <td>1.055792</td>\n",
       "      <td>2.146983</td>\n",
       "      <td>1.958707</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>1.901989</td>\n",
       "      <td>2.048484</td>\n",
       "      <td>2.419360</td>\n",
       "      <td>2.686230</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.444890</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>7.962172</td>\n",
       "      <td>253.987100</td>\n",
       "      <td>2.102171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036459</td>\n",
       "      <td>0.058015</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.005962</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>0.190465</td>\n",
       "      <td>0.012715</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.014601</td>\n",
       "      <td>0.037167</td>\n",
       "      <td>0.097775</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.089456</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>0.014937</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.025042</td>\n",
       "      <td>0.010896</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>0.022836</td>\n",
       "      <td>0.020646</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.010407</td>\n",
       "      <td>0.066587</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>0.056954</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>0.007528</td>\n",
       "      <td>0.013691</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.019905</td>\n",
       "      <td>0.054496</td>\n",
       "      <td>5.636837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22815.883089</td>\n",
       "      <td>0.160191</td>\n",
       "      <td>5.072885</td>\n",
       "      <td>0.282562</td>\n",
       "      <td>0.083107</td>\n",
       "      <td>0.739103</td>\n",
       "      <td>0.291949</td>\n",
       "      <td>0.197190</td>\n",
       "      <td>0.074239</td>\n",
       "      <td>0.089037</td>\n",
       "      <td>0.122213</td>\n",
       "      <td>0.082347</td>\n",
       "      <td>4.227082</td>\n",
       "      <td>0.715034</td>\n",
       "      <td>0.032816</td>\n",
       "      <td>0.350033</td>\n",
       "      <td>0.349551</td>\n",
       "      <td>0.417939</td>\n",
       "      <td>0.085858</td>\n",
       "      <td>2.674536</td>\n",
       "      <td>0.320627</td>\n",
       "      <td>0.231566</td>\n",
       "      <td>0.491688</td>\n",
       "      <td>0.274915</td>\n",
       "      <td>0.445195</td>\n",
       "      <td>0.329328</td>\n",
       "      <td>0.989139</td>\n",
       "      <td>0.945739</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>0.971223</td>\n",
       "      <td>0.755149</td>\n",
       "      <td>0.509577</td>\n",
       "      <td>0.483159</td>\n",
       "      <td>0.154959</td>\n",
       "      <td>0.140187</td>\n",
       "      <td>0.163012</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>13.027697</td>\n",
       "      <td>178.621154</td>\n",
       "      <td>0.303098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187432</td>\n",
       "      <td>0.233774</td>\n",
       "      <td>0.099515</td>\n",
       "      <td>0.076981</td>\n",
       "      <td>0.088239</td>\n",
       "      <td>0.392671</td>\n",
       "      <td>0.112040</td>\n",
       "      <td>0.095275</td>\n",
       "      <td>0.086244</td>\n",
       "      <td>0.095967</td>\n",
       "      <td>0.089821</td>\n",
       "      <td>0.119949</td>\n",
       "      <td>0.189172</td>\n",
       "      <td>0.297013</td>\n",
       "      <td>0.136155</td>\n",
       "      <td>0.285404</td>\n",
       "      <td>0.115145</td>\n",
       "      <td>0.108237</td>\n",
       "      <td>0.121304</td>\n",
       "      <td>0.107780</td>\n",
       "      <td>0.156253</td>\n",
       "      <td>0.103813</td>\n",
       "      <td>0.143947</td>\n",
       "      <td>0.149380</td>\n",
       "      <td>0.142198</td>\n",
       "      <td>0.083007</td>\n",
       "      <td>0.101485</td>\n",
       "      <td>0.249307</td>\n",
       "      <td>0.082405</td>\n",
       "      <td>0.116066</td>\n",
       "      <td>0.231757</td>\n",
       "      <td>0.099764</td>\n",
       "      <td>0.208479</td>\n",
       "      <td>0.102937</td>\n",
       "      <td>0.086436</td>\n",
       "      <td>0.116207</td>\n",
       "      <td>0.091737</td>\n",
       "      <td>0.139676</td>\n",
       "      <td>0.226995</td>\n",
       "      <td>2.456833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19780.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.225941</td>\n",
       "      <td>0.385517</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39487.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.288703</td>\n",
       "      <td>0.451349</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59211.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.345188</td>\n",
       "      <td>0.532858</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79146.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943662</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>648.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id      ...           Response\n",
       "count  59381.000000      ...       59381.000000\n",
       "mean   39507.211515      ...           5.636837\n",
       "std    22815.883089      ...           2.456833\n",
       "min        2.000000      ...           1.000000\n",
       "25%    19780.000000      ...           4.000000\n",
       "50%    39487.000000      ...           6.000000\n",
       "75%    59211.000000      ...           8.000000\n",
       "max    79146.000000      ...           8.000000\n",
       "\n",
       "[8 rows x 127 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Outliers detection\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "de93f695e88d8e320bf0efab985b9a461effc7ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6c0ef59358>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Responce variable\n",
    "aixs1 = plt.subplots(1,1,figsize=(10,5))\n",
    "sns.countplot(x='Response',data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical codes\n",
    "train['Product_Info_2'] = train['Product_Info_2'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_count_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employment_Info_1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.031997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employment_Info_4</td>\n",
       "      <td>6779</td>\n",
       "      <td>11.416110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Employment_Info_6</td>\n",
       "      <td>10854</td>\n",
       "      <td>18.278574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insurance_History_5</td>\n",
       "      <td>25396</td>\n",
       "      <td>42.767889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Family_Hist_2</td>\n",
       "      <td>28656</td>\n",
       "      <td>48.257860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Family_Hist_3</td>\n",
       "      <td>34241</td>\n",
       "      <td>57.663226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Family_Hist_4</td>\n",
       "      <td>19184</td>\n",
       "      <td>32.306630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Family_Hist_5</td>\n",
       "      <td>41811</td>\n",
       "      <td>70.411411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Medical_History_1</td>\n",
       "      <td>8889</td>\n",
       "      <td>14.969435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Medical_History_10</td>\n",
       "      <td>58824</td>\n",
       "      <td>99.061990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Medical_History_15</td>\n",
       "      <td>44596</td>\n",
       "      <td>75.101463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Medical_History_24</td>\n",
       "      <td>55580</td>\n",
       "      <td>93.598963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Medical_History_32</td>\n",
       "      <td>58274</td>\n",
       "      <td>98.135767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               features            ...             missing_count_percentage\n",
       "0     Employment_Info_1            ...                             0.031997\n",
       "1     Employment_Info_4            ...                            11.416110\n",
       "2     Employment_Info_6            ...                            18.278574\n",
       "3   Insurance_History_5            ...                            42.767889\n",
       "4         Family_Hist_2            ...                            48.257860\n",
       "5         Family_Hist_3            ...                            57.663226\n",
       "6         Family_Hist_4            ...                            32.306630\n",
       "7         Family_Hist_5            ...                            70.411411\n",
       "8     Medical_History_1            ...                            14.969435\n",
       "9    Medical_History_10            ...                            99.061990\n",
       "10   Medical_History_15            ...                            75.101463\n",
       "11   Medical_History_24            ...                            93.598963\n",
       "12   Medical_History_32            ...                            98.135767\n",
       "\n",
       "[13 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values\n",
    "train_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns containing missing values more than 80%\n",
    "train = train.drop(['Medical_History_10','Medical_History_24','Medical_History_32'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_count_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Employment_Info_1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.031997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Employment_Info_4</td>\n",
       "      <td>6779</td>\n",
       "      <td>11.416110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Employment_Info_6</td>\n",
       "      <td>10854</td>\n",
       "      <td>18.278574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insurance_History_5</td>\n",
       "      <td>25396</td>\n",
       "      <td>42.767889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Family_Hist_2</td>\n",
       "      <td>28656</td>\n",
       "      <td>48.257860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Family_Hist_3</td>\n",
       "      <td>34241</td>\n",
       "      <td>57.663226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Family_Hist_4</td>\n",
       "      <td>19184</td>\n",
       "      <td>32.306630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Family_Hist_5</td>\n",
       "      <td>41811</td>\n",
       "      <td>70.411411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Medical_History_1</td>\n",
       "      <td>8889</td>\n",
       "      <td>14.969435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Medical_History_15</td>\n",
       "      <td>44596</td>\n",
       "      <td>75.101463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features            ...             missing_count_percentage\n",
       "0    Employment_Info_1            ...                             0.031997\n",
       "1    Employment_Info_4            ...                            11.416110\n",
       "2    Employment_Info_6            ...                            18.278574\n",
       "3  Insurance_History_5            ...                            42.767889\n",
       "4        Family_Hist_2            ...                            48.257860\n",
       "5        Family_Hist_3            ...                            57.663226\n",
       "6        Family_Hist_4            ...                            32.306630\n",
       "7        Family_Hist_5            ...                            70.411411\n",
       "8    Medical_History_1            ...                            14.969435\n",
       "9   Medical_History_15            ...                            75.101463\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing values AGAIN\n",
    "train_missing= train.isnull().sum()[train.isnull().sum() !=0]\n",
    "train_missing=pd.DataFrame(train_missing.reset_index())\n",
    "train_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\n",
    "train_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100\n",
    "train_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Imputation fro continous variables\n",
    "Continuos = ['Employment_Info_1','Employment_Info_4', 'Employment_Info_6', 'Insurance_History_5',\n",
    "                    'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']\n",
    "train[Continuos] = train[Continuos].fillna(train[Continuos].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mode Imputation fro continous variables\n",
    "Categorical = ['Medical_History_1', 'Medical_History_15']\n",
    "train[Categorical] = train[Categorical].apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_count_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [features, missing_count, missing_count_percentage]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing values again\n",
    "train_missing= train.isnull().sum()[train.isnull().sum() !=0]\n",
    "train_missing=pd.DataFrame(train_missing.reset_index())\n",
    "train_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\n",
    "train_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100\n",
    "train_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_2</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>InsuredInfo_1</th>\n",
       "      <th>InsuredInfo_2</th>\n",
       "      <th>InsuredInfo_3</th>\n",
       "      <th>InsuredInfo_4</th>\n",
       "      <th>InsuredInfo_5</th>\n",
       "      <th>InsuredInfo_6</th>\n",
       "      <th>InsuredInfo_7</th>\n",
       "      <th>Insurance_History_1</th>\n",
       "      <th>Insurance_History_2</th>\n",
       "      <th>Insurance_History_3</th>\n",
       "      <th>Insurance_History_4</th>\n",
       "      <th>Insurance_History_5</th>\n",
       "      <th>Insurance_History_7</th>\n",
       "      <th>Insurance_History_8</th>\n",
       "      <th>Insurance_History_9</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>Family_Hist_4</th>\n",
       "      <th>Family_Hist_5</th>\n",
       "      <th>Medical_History_1</th>\n",
       "      <th>Medical_History_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_10</th>\n",
       "      <th>Medical_Keyword_11</th>\n",
       "      <th>Medical_Keyword_12</th>\n",
       "      <th>Medical_Keyword_13</th>\n",
       "      <th>Medical_Keyword_14</th>\n",
       "      <th>Medical_Keyword_15</th>\n",
       "      <th>Medical_Keyword_16</th>\n",
       "      <th>Medical_Keyword_17</th>\n",
       "      <th>Medical_Keyword_18</th>\n",
       "      <th>Medical_Keyword_19</th>\n",
       "      <th>Medical_Keyword_20</th>\n",
       "      <th>Medical_Keyword_21</th>\n",
       "      <th>Medical_Keyword_22</th>\n",
       "      <th>Medical_Keyword_23</th>\n",
       "      <th>Medical_Keyword_24</th>\n",
       "      <th>Medical_Keyword_25</th>\n",
       "      <th>Medical_Keyword_26</th>\n",
       "      <th>Medical_Keyword_27</th>\n",
       "      <th>Medical_Keyword_28</th>\n",
       "      <th>Medical_Keyword_29</th>\n",
       "      <th>Medical_Keyword_30</th>\n",
       "      <th>Medical_Keyword_31</th>\n",
       "      <th>Medical_Keyword_32</th>\n",
       "      <th>Medical_Keyword_33</th>\n",
       "      <th>Medical_Keyword_34</th>\n",
       "      <th>Medical_Keyword_35</th>\n",
       "      <th>Medical_Keyword_36</th>\n",
       "      <th>Medical_Keyword_37</th>\n",
       "      <th>Medical_Keyword_38</th>\n",
       "      <th>Medical_Keyword_39</th>\n",
       "      <th>Medical_Keyword_40</th>\n",
       "      <th>Medical_Keyword_41</th>\n",
       "      <th>Medical_Keyword_42</th>\n",
       "      <th>Medical_Keyword_43</th>\n",
       "      <th>Medical_Keyword_44</th>\n",
       "      <th>Medical_Keyword_45</th>\n",
       "      <th>Medical_Keyword_46</th>\n",
       "      <th>Medical_Keyword_47</th>\n",
       "      <th>Medical_Keyword_48</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.148536</td>\n",
       "      <td>0.323008</td>\n",
       "      <td>0.028</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.444890</td>\n",
       "      <td>0.526786</td>\n",
       "      <td>4.0</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.131799</td>\n",
       "      <td>0.272288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>5.0</td>\n",
       "      <td>412</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.288703</td>\n",
       "      <td>0.428780</td>\n",
       "      <td>0.030</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.205021</td>\n",
       "      <td>0.352438</td>\n",
       "      <td>0.042</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.234310</td>\n",
       "      <td>0.424046</td>\n",
       "      <td>0.027</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Product_Info_1    ...     Medical_Keyword_48  Response\n",
       "0   2               1    ...                      0         8\n",
       "1   5               1    ...                      0         4\n",
       "2   6               1    ...                      0         8\n",
       "3   7               1    ...                      0         8\n",
       "4   8               1    ...                      0         8\n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling\n",
    "\n",
    "1)Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50473, 125)\n",
      "(8908, 125)\n"
     ]
    }
   ],
   "source": [
    "#Dataset split\n",
    "train_data, test_data = train_test_split(train, test_size = 0.15)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_2</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>InsuredInfo_1</th>\n",
       "      <th>InsuredInfo_2</th>\n",
       "      <th>InsuredInfo_3</th>\n",
       "      <th>InsuredInfo_4</th>\n",
       "      <th>InsuredInfo_5</th>\n",
       "      <th>InsuredInfo_6</th>\n",
       "      <th>InsuredInfo_7</th>\n",
       "      <th>Insurance_History_1</th>\n",
       "      <th>Insurance_History_2</th>\n",
       "      <th>Insurance_History_3</th>\n",
       "      <th>Insurance_History_4</th>\n",
       "      <th>Insurance_History_5</th>\n",
       "      <th>Insurance_History_7</th>\n",
       "      <th>Insurance_History_8</th>\n",
       "      <th>Insurance_History_9</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>Family_Hist_4</th>\n",
       "      <th>Family_Hist_5</th>\n",
       "      <th>Medical_History_1</th>\n",
       "      <th>Medical_History_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_10</th>\n",
       "      <th>Medical_Keyword_11</th>\n",
       "      <th>Medical_Keyword_12</th>\n",
       "      <th>Medical_Keyword_13</th>\n",
       "      <th>Medical_Keyword_14</th>\n",
       "      <th>Medical_Keyword_15</th>\n",
       "      <th>Medical_Keyword_16</th>\n",
       "      <th>Medical_Keyword_17</th>\n",
       "      <th>Medical_Keyword_18</th>\n",
       "      <th>Medical_Keyword_19</th>\n",
       "      <th>Medical_Keyword_20</th>\n",
       "      <th>Medical_Keyword_21</th>\n",
       "      <th>Medical_Keyword_22</th>\n",
       "      <th>Medical_Keyword_23</th>\n",
       "      <th>Medical_Keyword_24</th>\n",
       "      <th>Medical_Keyword_25</th>\n",
       "      <th>Medical_Keyword_26</th>\n",
       "      <th>Medical_Keyword_27</th>\n",
       "      <th>Medical_Keyword_28</th>\n",
       "      <th>Medical_Keyword_29</th>\n",
       "      <th>Medical_Keyword_30</th>\n",
       "      <th>Medical_Keyword_31</th>\n",
       "      <th>Medical_Keyword_32</th>\n",
       "      <th>Medical_Keyword_33</th>\n",
       "      <th>Medical_Keyword_34</th>\n",
       "      <th>Medical_Keyword_35</th>\n",
       "      <th>Medical_Keyword_36</th>\n",
       "      <th>Medical_Keyword_37</th>\n",
       "      <th>Medical_Keyword_38</th>\n",
       "      <th>Medical_Keyword_39</th>\n",
       "      <th>Medical_Keyword_40</th>\n",
       "      <th>Medical_Keyword_41</th>\n",
       "      <th>Medical_Keyword_42</th>\n",
       "      <th>Medical_Keyword_43</th>\n",
       "      <th>Medical_Keyword_44</th>\n",
       "      <th>Medical_Keyword_45</th>\n",
       "      <th>Medical_Keyword_46</th>\n",
       "      <th>Medical_Keyword_47</th>\n",
       "      <th>Medical_Keyword_48</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37724</th>\n",
       "      <td>50086</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.261506</td>\n",
       "      <td>0.455416</td>\n",
       "      <td>0.100</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>0.444890</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>478</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33550</th>\n",
       "      <td>44541</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.236402</td>\n",
       "      <td>0.344016</td>\n",
       "      <td>0.100</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31838</th>\n",
       "      <td>42260</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.236402</td>\n",
       "      <td>0.533794</td>\n",
       "      <td>0.080</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.444890</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>3.0</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>11130</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.497908</td>\n",
       "      <td>0.715829</td>\n",
       "      <td>0.085</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>3.0</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36028</th>\n",
       "      <td>47841</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.309623</td>\n",
       "      <td>0.410825</td>\n",
       "      <td>0.060</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Product_Info_1    ...     Medical_Keyword_48  Response\n",
       "37724  50086               1    ...                      0         6\n",
       "33550  44541               1    ...                      0         3\n",
       "31838  42260               1    ...                      0         7\n",
       "8351   11130               1    ...                      1         5\n",
       "36028  47841               1    ...                      1         2\n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#traindata\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_2</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>InsuredInfo_1</th>\n",
       "      <th>InsuredInfo_2</th>\n",
       "      <th>InsuredInfo_3</th>\n",
       "      <th>InsuredInfo_4</th>\n",
       "      <th>InsuredInfo_5</th>\n",
       "      <th>InsuredInfo_6</th>\n",
       "      <th>InsuredInfo_7</th>\n",
       "      <th>Insurance_History_1</th>\n",
       "      <th>Insurance_History_2</th>\n",
       "      <th>Insurance_History_3</th>\n",
       "      <th>Insurance_History_4</th>\n",
       "      <th>Insurance_History_5</th>\n",
       "      <th>Insurance_History_7</th>\n",
       "      <th>Insurance_History_8</th>\n",
       "      <th>Insurance_History_9</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>Family_Hist_4</th>\n",
       "      <th>Family_Hist_5</th>\n",
       "      <th>Medical_History_1</th>\n",
       "      <th>Medical_History_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_10</th>\n",
       "      <th>Medical_Keyword_11</th>\n",
       "      <th>Medical_Keyword_12</th>\n",
       "      <th>Medical_Keyword_13</th>\n",
       "      <th>Medical_Keyword_14</th>\n",
       "      <th>Medical_Keyword_15</th>\n",
       "      <th>Medical_Keyword_16</th>\n",
       "      <th>Medical_Keyword_17</th>\n",
       "      <th>Medical_Keyword_18</th>\n",
       "      <th>Medical_Keyword_19</th>\n",
       "      <th>Medical_Keyword_20</th>\n",
       "      <th>Medical_Keyword_21</th>\n",
       "      <th>Medical_Keyword_22</th>\n",
       "      <th>Medical_Keyword_23</th>\n",
       "      <th>Medical_Keyword_24</th>\n",
       "      <th>Medical_Keyword_25</th>\n",
       "      <th>Medical_Keyword_26</th>\n",
       "      <th>Medical_Keyword_27</th>\n",
       "      <th>Medical_Keyword_28</th>\n",
       "      <th>Medical_Keyword_29</th>\n",
       "      <th>Medical_Keyword_30</th>\n",
       "      <th>Medical_Keyword_31</th>\n",
       "      <th>Medical_Keyword_32</th>\n",
       "      <th>Medical_Keyword_33</th>\n",
       "      <th>Medical_Keyword_34</th>\n",
       "      <th>Medical_Keyword_35</th>\n",
       "      <th>Medical_Keyword_36</th>\n",
       "      <th>Medical_Keyword_37</th>\n",
       "      <th>Medical_Keyword_38</th>\n",
       "      <th>Medical_Keyword_39</th>\n",
       "      <th>Medical_Keyword_40</th>\n",
       "      <th>Medical_Keyword_41</th>\n",
       "      <th>Medical_Keyword_42</th>\n",
       "      <th>Medical_Keyword_43</th>\n",
       "      <th>Medical_Keyword_44</th>\n",
       "      <th>Medical_Keyword_45</th>\n",
       "      <th>Medical_Keyword_46</th>\n",
       "      <th>Medical_Keyword_47</th>\n",
       "      <th>Medical_Keyword_48</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42541</th>\n",
       "      <td>56547</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.215481</td>\n",
       "      <td>0.487645</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>13.0</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37399</th>\n",
       "      <td>49660</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.205021</td>\n",
       "      <td>0.386332</td>\n",
       "      <td>0.030</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>5.0</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37016</th>\n",
       "      <td>49150</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.194561</td>\n",
       "      <td>0.289568</td>\n",
       "      <td>0.100</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>2</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>10.0</td>\n",
       "      <td>161</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58288</th>\n",
       "      <td>77693</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.215481</td>\n",
       "      <td>0.388655</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>30.0</td>\n",
       "      <td>491</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54857</th>\n",
       "      <td>73049</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.516736</td>\n",
       "      <td>0.856842</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Product_Info_1    ...     Medical_Keyword_48  Response\n",
       "42541  56547               1    ...                      0         8\n",
       "37399  49660               1    ...                      0         6\n",
       "37016  49150               1    ...                      0         8\n",
       "58288  77693               1    ...                      0         8\n",
       "54857  73049               1    ...                      0         2\n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#traindata\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50473, 123)\n",
      "(50473,)\n",
      "(8908, 123)\n",
      "(8908,)\n"
     ]
    }
   ],
   "source": [
    "#Predictor and responce variables\n",
    "train_x = train_data.drop(['Id', 'Response'], axis=1)\n",
    "train_y = train_data['Response']\n",
    "test_x = test_data.drop(['Id', 'Response'], axis=1)\n",
    "test_y = test_data['Response']\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37724    6\n",
       "33550    3\n",
       "31838    7\n",
       "8351     5\n",
       "36028    2\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train responce\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42541    8\n",
       "37399    6\n",
       "37016    8\n",
       "58288    8\n",
       "54857    2\n",
       "Name: Response, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test responce\n",
    "test_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50473, 123)\n",
      "(50473, 8)\n",
      "(8908, 123)\n",
      "(8908,)\n"
     ]
    }
   ],
   "source": [
    "#converting to responce categorical class labels(0-7)\n",
    "train_y = train_y-1\n",
    "train_y = to_categorical(train_y, num_classes= 8)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for normalization\n",
    "def normalization(data):\n",
    "    return (data - data.min())/(data.max() - data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data\n",
    "train_x = normalization(train_x)\n",
    "test_x = normalization(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_2</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>InsuredInfo_1</th>\n",
       "      <th>InsuredInfo_2</th>\n",
       "      <th>InsuredInfo_3</th>\n",
       "      <th>InsuredInfo_4</th>\n",
       "      <th>InsuredInfo_5</th>\n",
       "      <th>InsuredInfo_6</th>\n",
       "      <th>InsuredInfo_7</th>\n",
       "      <th>Insurance_History_1</th>\n",
       "      <th>Insurance_History_2</th>\n",
       "      <th>Insurance_History_3</th>\n",
       "      <th>Insurance_History_4</th>\n",
       "      <th>Insurance_History_5</th>\n",
       "      <th>Insurance_History_7</th>\n",
       "      <th>Insurance_History_8</th>\n",
       "      <th>Insurance_History_9</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>Family_Hist_4</th>\n",
       "      <th>Family_Hist_5</th>\n",
       "      <th>Medical_History_1</th>\n",
       "      <th>Medical_History_2</th>\n",
       "      <th>Medical_History_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_9</th>\n",
       "      <th>Medical_Keyword_10</th>\n",
       "      <th>Medical_Keyword_11</th>\n",
       "      <th>Medical_Keyword_12</th>\n",
       "      <th>Medical_Keyword_13</th>\n",
       "      <th>Medical_Keyword_14</th>\n",
       "      <th>Medical_Keyword_15</th>\n",
       "      <th>Medical_Keyword_16</th>\n",
       "      <th>Medical_Keyword_17</th>\n",
       "      <th>Medical_Keyword_18</th>\n",
       "      <th>Medical_Keyword_19</th>\n",
       "      <th>Medical_Keyword_20</th>\n",
       "      <th>Medical_Keyword_21</th>\n",
       "      <th>Medical_Keyword_22</th>\n",
       "      <th>Medical_Keyword_23</th>\n",
       "      <th>Medical_Keyword_24</th>\n",
       "      <th>Medical_Keyword_25</th>\n",
       "      <th>Medical_Keyword_26</th>\n",
       "      <th>Medical_Keyword_27</th>\n",
       "      <th>Medical_Keyword_28</th>\n",
       "      <th>Medical_Keyword_29</th>\n",
       "      <th>Medical_Keyword_30</th>\n",
       "      <th>Medical_Keyword_31</th>\n",
       "      <th>Medical_Keyword_32</th>\n",
       "      <th>Medical_Keyword_33</th>\n",
       "      <th>Medical_Keyword_34</th>\n",
       "      <th>Medical_Keyword_35</th>\n",
       "      <th>Medical_Keyword_36</th>\n",
       "      <th>Medical_Keyword_37</th>\n",
       "      <th>Medical_Keyword_38</th>\n",
       "      <th>Medical_Keyword_39</th>\n",
       "      <th>Medical_Keyword_40</th>\n",
       "      <th>Medical_Keyword_41</th>\n",
       "      <th>Medical_Keyword_42</th>\n",
       "      <th>Medical_Keyword_43</th>\n",
       "      <th>Medical_Keyword_44</th>\n",
       "      <th>Medical_Keyword_45</th>\n",
       "      <th>Medical_Keyword_46</th>\n",
       "      <th>Medical_Keyword_47</th>\n",
       "      <th>Medical_Keyword_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37724</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.261506</td>\n",
       "      <td>0.455416</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>0.471451</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.737249</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.236402</td>\n",
       "      <td>0.344016</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.497737</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.171561</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31838</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.236402</td>\n",
       "      <td>0.533794</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.471451</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.171561</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.497908</td>\n",
       "      <td>0.715829</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.757342</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36028</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.309623</td>\n",
       "      <td>0.410825</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.474550</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.484635</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.171561</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Product_Info_1         ...          Medical_Keyword_48\n",
       "37724             0.0         ...                         0.0\n",
       "33550             0.0         ...                         0.0\n",
       "31838             0.0         ...                         0.0\n",
       "8351              0.0         ...                         1.0\n",
       "36028             0.0         ...                         1.0\n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#traindata\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Info_1</th>\n",
       "      <th>Product_Info_2</th>\n",
       "      <th>Product_Info_3</th>\n",
       "      <th>Product_Info_4</th>\n",
       "      <th>Product_Info_5</th>\n",
       "      <th>Product_Info_6</th>\n",
       "      <th>Product_Info_7</th>\n",
       "      <th>Ins_Age</th>\n",
       "      <th>Ht</th>\n",
       "      <th>Wt</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Employment_Info_1</th>\n",
       "      <th>Employment_Info_2</th>\n",
       "      <th>Employment_Info_3</th>\n",
       "      <th>Employment_Info_4</th>\n",
       "      <th>Employment_Info_5</th>\n",
       "      <th>Employment_Info_6</th>\n",
       "      <th>InsuredInfo_1</th>\n",
       "      <th>InsuredInfo_2</th>\n",
       "      <th>InsuredInfo_3</th>\n",
       "      <th>InsuredInfo_4</th>\n",
       "      <th>InsuredInfo_5</th>\n",
       "      <th>InsuredInfo_6</th>\n",
       "      <th>InsuredInfo_7</th>\n",
       "      <th>Insurance_History_1</th>\n",
       "      <th>Insurance_History_2</th>\n",
       "      <th>Insurance_History_3</th>\n",
       "      <th>Insurance_History_4</th>\n",
       "      <th>Insurance_History_5</th>\n",
       "      <th>Insurance_History_7</th>\n",
       "      <th>Insurance_History_8</th>\n",
       "      <th>Insurance_History_9</th>\n",
       "      <th>Family_Hist_1</th>\n",
       "      <th>Family_Hist_2</th>\n",
       "      <th>Family_Hist_3</th>\n",
       "      <th>Family_Hist_4</th>\n",
       "      <th>Family_Hist_5</th>\n",
       "      <th>Medical_History_1</th>\n",
       "      <th>Medical_History_2</th>\n",
       "      <th>Medical_History_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Medical_Keyword_9</th>\n",
       "      <th>Medical_Keyword_10</th>\n",
       "      <th>Medical_Keyword_11</th>\n",
       "      <th>Medical_Keyword_12</th>\n",
       "      <th>Medical_Keyword_13</th>\n",
       "      <th>Medical_Keyword_14</th>\n",
       "      <th>Medical_Keyword_15</th>\n",
       "      <th>Medical_Keyword_16</th>\n",
       "      <th>Medical_Keyword_17</th>\n",
       "      <th>Medical_Keyword_18</th>\n",
       "      <th>Medical_Keyword_19</th>\n",
       "      <th>Medical_Keyword_20</th>\n",
       "      <th>Medical_Keyword_21</th>\n",
       "      <th>Medical_Keyword_22</th>\n",
       "      <th>Medical_Keyword_23</th>\n",
       "      <th>Medical_Keyword_24</th>\n",
       "      <th>Medical_Keyword_25</th>\n",
       "      <th>Medical_Keyword_26</th>\n",
       "      <th>Medical_Keyword_27</th>\n",
       "      <th>Medical_Keyword_28</th>\n",
       "      <th>Medical_Keyword_29</th>\n",
       "      <th>Medical_Keyword_30</th>\n",
       "      <th>Medical_Keyword_31</th>\n",
       "      <th>Medical_Keyword_32</th>\n",
       "      <th>Medical_Keyword_33</th>\n",
       "      <th>Medical_Keyword_34</th>\n",
       "      <th>Medical_Keyword_35</th>\n",
       "      <th>Medical_Keyword_36</th>\n",
       "      <th>Medical_Keyword_37</th>\n",
       "      <th>Medical_Keyword_38</th>\n",
       "      <th>Medical_Keyword_39</th>\n",
       "      <th>Medical_Keyword_40</th>\n",
       "      <th>Medical_Keyword_41</th>\n",
       "      <th>Medical_Keyword_42</th>\n",
       "      <th>Medical_Keyword_43</th>\n",
       "      <th>Medical_Keyword_44</th>\n",
       "      <th>Medical_Keyword_45</th>\n",
       "      <th>Medical_Keyword_46</th>\n",
       "      <th>Medical_Keyword_47</th>\n",
       "      <th>Medical_Keyword_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42541</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.391839</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.547969</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.653965</td>\n",
       "      <td>0.054393</td>\n",
       "      <td>0.172360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37399</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.190341</td>\n",
       "      <td>0.271582</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.547969</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.653965</td>\n",
       "      <td>0.020921</td>\n",
       "      <td>0.172360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37016</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.176136</td>\n",
       "      <td>0.156723</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.547969</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.653965</td>\n",
       "      <td>0.041841</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58288</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.274339</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.051992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.547969</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.653965</td>\n",
       "      <td>0.125523</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54857</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.830073</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361469</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.051992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.547969</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.653965</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.172360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Product_Info_1         ...          Medical_Keyword_48\n",
       "42541             0.0         ...                         0.0\n",
       "37399             0.0         ...                         0.0\n",
       "37016             0.0         ...                         0.0\n",
       "58288             0.0         ...                         0.0\n",
       "54857             0.0         ...                         0.0\n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testdata\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3)Models and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50473, 123)\n",
      "(8908, 123)\n"
     ]
    }
   ],
   "source": [
    "#Train and test data shapes\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning static parameter\n",
    "nb_epoch = 20\n",
    "batch_size = 512\n",
    "input_dim = train_x.shape[1]\n",
    "hidden_dim1 = 64 \n",
    "hidden_dim2 = 32\n",
    "hidden_dim3 = 16\n",
    "learning_rate = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for auto encoder to get and fit model\n",
    "def get_fit_encoder(xs_train,xs_cv,test_x):\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "    encoder = Dense(input_dim, activation=\"relu\",activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "    \n",
    "    encoder = Dense(hidden_dim1, activation=\"relu\")(encoder)\n",
    "    encoder = Dense(hidden_dim2, activation=\"relu\")(encoder)\n",
    "    encoder = Dense(hidden_dim3, activation=\"relu\", name=\"encoder\")(encoder)\n",
    "    \n",
    "    decoder = Dense(hidden_dim3, activation=\"relu\")(encoder)\n",
    "    decoder = Dense(hidden_dim2, activation='relu')(decoder)\n",
    "    decoder = Dense(hidden_dim1, activation='relu')(decoder)\n",
    "    \n",
    "    decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    #autoencoder.summary()\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                        loss='binary_crossentropy')\n",
    "    \n",
    "    history = autoencoder.fit(x=xs_train, y=xs_train,\n",
    "                          epochs=nb_epoch,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(xs_cv, xs_cv),\n",
    "                          verbose=1)\n",
    "    encoder = Model(autoencoder.input, autoencoder.get_layer('encoder').output)\n",
    "    x_auto_train= encoder.predict(xs_train)\n",
    "    x_auto_cv= encoder.predict(xs_cv)\n",
    "    x_auto_test= encoder.predict(test_x)\n",
    "    return x_auto_train,x_auto_cv,x_auto_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Neural network to get and fit model\n",
    "def get_fit_neuralnetwork(xs_encoder_train,xs_encoder_cv,xs_encoder_test,ys_train,ys_cv):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim = input_dim , init = 'uniform', activation = 'relu', input_dim = 16))\n",
    "    classifier.add(Dense(output_dim = 16 , init = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 8 , init = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'softmax'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    cp = ModelCheckpoint(filepath=\"autoencoder_data.h5\",\n",
    "                         save_best_only=True,\n",
    "                         verbose=0)\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                     histogram_freq=0,\n",
    "                     write_graph=True,\n",
    "                     write_images=True)\n",
    "    history = classifier.fit(xs_encoder_train, ys_train,\n",
    "                             batch_size=batch_size ,\n",
    "                             epochs=nb_epoch ,\n",
    "                             shuffle=True,\n",
    "                             validation_data=(xs_encoder_cv,ys_cv),\n",
    "                             verbose=1,\n",
    "                            callbacks=[cp, tb]).history\n",
    "    y_pred_NN = classifier.predict(xs_encoder_test, batch_size=batch_size, verbose=1)\n",
    "    y_pred_NN = np.argmax(y_pred_NN,axis = 1) + 1\n",
    "    return y_pred_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for State of art model to get and fit model\n",
    "def get_fit_SOA_Models(x_sampletrain,y_sampletrain,test_x):\n",
    "    model1 = RandomForestClassifier()\n",
    "    \n",
    "    inside_train_y = np.argmax(y_sampletrain, axis = 1) + 1   \n",
    "    \n",
    "    model1.fit(x_sampletrain, inside_train_y)\n",
    "    \n",
    "    y_pred1 = model1.predict(test_x) \n",
    "    return y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for model evaluation\n",
    "def model_evaluation (test_y,y_pred_NN,y_pred1):\n",
    "   \n",
    "    accuracy_NN = accuracy_score(test_y,y_pred_NN)\n",
    "    F1_score_NN=f1_score(test_y, y_pred_NN,average='weighted')\n",
    "    Precision_NN=precision_score(test_y, y_pred_NN,average='weighted')\n",
    "    Recall_score_NN=recall_score(test_y, y_pred_NN,average='weighted')\n",
    "    \n",
    "    accuracy_SOAM1 = accuracy_score(test_y, y_pred1)\n",
    "    F1_score_SOAM1=f1_score(test_y, y_pred1,average='weighted')\n",
    "    Precision_SOAM1=precision_score(test_y, y_pred1,average='weighted')\n",
    "    Recall_score_SOAM1=recall_score(test_y, y_pred1,average='weighted')\n",
    "    \n",
    "    \n",
    "    print(\"Classification score for NN:\", classification_report(test_y,y_pred_NN))\n",
    "    print(\"Classification score for SOAM1:\", classification_report(test_y, y_pred1))\n",
    "       \n",
    "    return accuracy_NN,F1_score_NN,Precision_NN,Recall_score_NN,accuracy_SOAM1,F1_score_SOAM1,Precision_SOAM1,Recall_score_SOAM1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "#Function to pass sample data to autoencoder and neural network functions\n",
    "def data_sampling(train_x, train_y, test_x, test_y):\n",
    "    accuracy_list_NN= []\n",
    "    F1_score_list_NN=[]\n",
    "    Precision_list_NN=[]\n",
    "    Recall_list_NN=[]\n",
    "    \n",
    "    accuracy_list_SOAM1= []\n",
    "    F1_score_list_SOAM1=[]\n",
    "    Precision_list_SOAM1=[]\n",
    "    Recall_list_SOAM1=[]\n",
    "    \n",
    "    \n",
    "    for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7, 0.8, 0.9, 0.99]:\n",
    "        print(\"data sample {}\".format(i*100))\n",
    "        x_sampletrain, _, y_sampletrain, _ = train_test_split(train_x, train_y, stratify= train_y, train_size=i)\n",
    "        xs_train, xs_cv, ys_train, ys_cv = train_test_split(x_sampletrain, y_sampletrain, stratify=y_sampletrain, train_size=0.9)\n",
    "        xs_train.shape, xs_cv.shape, ys_train.shape, ys_cv.shape\n",
    "        xs_encoder_train,xs_encoder_cv,xs_encoder_test=get_fit_encoder(xs_train,xs_cv,test_x)\n",
    "        y_pred_NN=get_fit_neuralnetwork(xs_encoder_train,xs_encoder_cv,xs_encoder_test,ys_train,ys_cv)\n",
    "        \n",
    "        y_pred1=get_fit_SOA_Models(x_sampletrain,y_sampletrain,test_x)\n",
    "        \n",
    "        accuracy_NN,F1_score_NN,Precision_NN,Recall_NN,accuracy_SOAM1,F1_score_SOAM1,Precision_SOAM1,Recall_SOAM1=model_evaluation(test_y,y_pred_NN,y_pred1)\n",
    "        \n",
    "        \n",
    "        accuracy_list_NN.append(accuracy_NN)\n",
    "        F1_score_list_NN.append(F1_score_NN)\n",
    "        Precision_list_NN.append(Precision_NN)\n",
    "        Recall_list_NN.append(Recall_NN)\n",
    "        \n",
    "        accuracy_list_SOAM1.append(accuracy_SOAM1)\n",
    "        F1_score_list_SOAM1.append(F1_score_SOAM1)\n",
    "        Precision_list_SOAM1.append(Precision_SOAM1)\n",
    "        Recall_list_SOAM1.append(Recall_SOAM1)\n",
    "        \n",
    "        \n",
    "    return accuracy_list_NN,F1_score_list_NN,Precision_list_NN,Recall_list_NN,accuracy_list_SOAM1,F1_score_list_SOAM1,Precision_list_SOAM1,Recall_list_SOAM1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data sample 10.0\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4542 samples, validate on 505 samples\n",
      "Epoch 1/20\n",
      "4542/4542 [==============================] - 2s 395us/step - loss: 0.6817 - val_loss: 0.6498\n",
      "Epoch 2/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.5763 - val_loss: 0.4489\n",
      "Epoch 3/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.3834 - val_loss: 0.3497\n",
      "Epoch 4/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.3169 - val_loss: 0.3068\n",
      "Epoch 5/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2999 - val_loss: 0.2999\n",
      "Epoch 6/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2935 - val_loss: 0.2970\n",
      "Epoch 7/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2905 - val_loss: 0.2947\n",
      "Epoch 8/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2887 - val_loss: 0.2932\n",
      "Epoch 9/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2872 - val_loss: 0.2920\n",
      "Epoch 10/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2860 - val_loss: 0.2910\n",
      "Epoch 11/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2853 - val_loss: 0.2904\n",
      "Epoch 12/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2848 - val_loss: 0.2900\n",
      "Epoch 13/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2844 - val_loss: 0.2896\n",
      "Epoch 14/20\n",
      "4542/4542 [==============================] - 0s 13us/step - loss: 0.2841 - val_loss: 0.2894\n",
      "Epoch 15/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2837 - val_loss: 0.2889\n",
      "Epoch 16/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2833 - val_loss: 0.2885\n",
      "Epoch 17/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2827 - val_loss: 0.2876\n",
      "Epoch 18/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2814 - val_loss: 0.2857\n",
      "Epoch 19/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2795 - val_loss: 0.2837\n",
      "Epoch 20/20\n",
      "4542/4542 [==============================] - 0s 12us/step - loss: 0.2770 - val_loss: 0.2808\n",
      "Train on 4542 samples, validate on 505 samples\n",
      "Epoch 1/20\n",
      "4542/4542 [==============================] - 0s 54us/step - loss: 2.0755 - acc: 0.3283 - val_loss: 2.0693 - val_acc: 0.3267\n",
      "Epoch 2/20\n",
      "4542/4542 [==============================] - 0s 9us/step - loss: 2.0601 - acc: 0.3276 - val_loss: 2.0434 - val_acc: 0.3267\n",
      "Epoch 3/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 2.0180 - acc: 0.3276 - val_loss: 1.9763 - val_acc: 0.3267\n",
      "Epoch 4/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.9281 - acc: 0.3276 - val_loss: 1.8653 - val_acc: 0.3267\n",
      "Epoch 5/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.8433 - acc: 0.3276 - val_loss: 1.8217 - val_acc: 0.3267\n",
      "Epoch 6/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.8155 - acc: 0.3276 - val_loss: 1.7945 - val_acc: 0.3267\n",
      "Epoch 7/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.7943 - acc: 0.3276 - val_loss: 1.7874 - val_acc: 0.3267\n",
      "Epoch 8/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.7864 - acc: 0.3276 - val_loss: 1.7793 - val_acc: 0.3267\n",
      "Epoch 9/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.7818 - acc: 0.3276 - val_loss: 1.7761 - val_acc: 0.3267\n",
      "Epoch 10/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.7802 - acc: 0.3276 - val_loss: 1.7747 - val_acc: 0.3267\n",
      "Epoch 11/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.7790 - acc: 0.3276 - val_loss: 1.7741 - val_acc: 0.3267\n",
      "Epoch 12/20\n",
      "4542/4542 [==============================] - 0s 7us/step - loss: 1.7783 - acc: 0.3276 - val_loss: 1.7733 - val_acc: 0.3267\n",
      "Epoch 13/20\n",
      "4542/4542 [==============================] - 0s 7us/step - loss: 1.7776 - acc: 0.3276 - val_loss: 1.7727 - val_acc: 0.3267\n",
      "Epoch 14/20\n",
      "4542/4542 [==============================] - 0s 8us/step - loss: 1.7772 - acc: 0.3276 - val_loss: 1.7725 - val_acc: 0.3267\n",
      "Epoch 15/20\n",
      "4542/4542 [==============================] - 0s 7us/step - loss: 1.7765 - acc: 0.3276 - val_loss: 1.7718 - val_acc: 0.3267\n",
      "Epoch 16/20\n",
      "4542/4542 [==============================] - 0s 7us/step - loss: 1.7761 - acc: 0.3276 - val_loss: 1.7712 - val_acc: 0.3267\n",
      "Epoch 17/20\n",
      "4542/4542 [==============================] - 0s 7us/step - loss: 1.7758 - acc: 0.3276 - val_loss: 1.7711 - val_acc: 0.3267\n",
      "Epoch 18/20\n",
      "4542/4542 [==============================] - 0s 7us/step - loss: 1.7759 - acc: 0.3276 - val_loss: 1.7712 - val_acc: 0.3267\n",
      "Epoch 19/20\n",
      "4542/4542 [==============================] - 0s 7us/step - loss: 1.7754 - acc: 0.3276 - val_loss: 1.7702 - val_acc: 0.3267\n",
      "Epoch 20/20\n",
      "4542/4542 [==============================] - 0s 7us/step - loss: 1.7745 - acc: 0.3276 - val_loss: 1.7706 - val_acc: 0.3267\n",
      "8908/8908 [==============================] - 0s 9us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       925\n",
      "           2       0.00      0.00      0.00       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.00      0.00      0.00       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.00      0.00      0.00      1625\n",
      "           7       0.00      0.00      0.00      1247\n",
      "           8       0.33      1.00      0.50      2959\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      8908\n",
      "   macro avg       0.04      0.12      0.06      8908\n",
      "weighted avg       0.11      0.33      0.17      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.21      0.22      0.21       925\n",
      "           2       0.20      0.20      0.20       975\n",
      "           3       0.33      0.29      0.31       150\n",
      "           4       0.45      0.37      0.41       199\n",
      "           5       0.46      0.22      0.30       828\n",
      "           6       0.29      0.27      0.28      1625\n",
      "           7       0.30      0.24      0.26      1247\n",
      "           8       0.60      0.77      0.68      2959\n",
      "\n",
      "   micro avg       0.42      0.42      0.42      8908\n",
      "   macro avg       0.35      0.32      0.33      8908\n",
      "weighted avg       0.40      0.42      0.40      8908\n",
      "\n",
      "data sample 20.0\n",
      "Train on 9084 samples, validate on 1010 samples\n",
      "Epoch 1/20\n",
      "9084/9084 [==============================] - 1s 86us/step - loss: 0.5991 - val_loss: 0.4027\n",
      "Epoch 2/20\n",
      "9084/9084 [==============================] - 0s 12us/step - loss: 0.3400 - val_loss: 0.3040\n",
      "Epoch 3/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2951 - val_loss: 0.2937\n",
      "Epoch 4/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2893 - val_loss: 0.2903\n",
      "Epoch 5/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2862 - val_loss: 0.2879\n",
      "Epoch 6/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2838 - val_loss: 0.2852\n",
      "Epoch 7/20\n",
      "9084/9084 [==============================] - 0s 12us/step - loss: 0.2788 - val_loss: 0.2775\n",
      "Epoch 8/20\n",
      "9084/9084 [==============================] - 0s 12us/step - loss: 0.2706 - val_loss: 0.2704\n",
      "Epoch 9/20\n",
      "9084/9084 [==============================] - 0s 12us/step - loss: 0.2646 - val_loss: 0.2658\n",
      "Epoch 10/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2606 - val_loss: 0.2617\n",
      "Epoch 11/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2567 - val_loss: 0.2578\n",
      "Epoch 12/20\n",
      "9084/9084 [==============================] - 0s 12us/step - loss: 0.2533 - val_loss: 0.2545\n",
      "Epoch 13/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2505 - val_loss: 0.2519\n",
      "Epoch 14/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2481 - val_loss: 0.2496\n",
      "Epoch 15/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2460 - val_loss: 0.2478\n",
      "Epoch 16/20\n",
      "9084/9084 [==============================] - 0s 12us/step - loss: 0.2443 - val_loss: 0.2460\n",
      "Epoch 17/20\n",
      "9084/9084 [==============================] - 0s 12us/step - loss: 0.2428 - val_loss: 0.2447\n",
      "Epoch 18/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2411 - val_loss: 0.2428\n",
      "Epoch 19/20\n",
      "9084/9084 [==============================] - 0s 12us/step - loss: 0.2391 - val_loss: 0.2404\n",
      "Epoch 20/20\n",
      "9084/9084 [==============================] - 0s 11us/step - loss: 0.2363 - val_loss: 0.2376\n",
      "Train on 9084 samples, validate on 1010 samples\n",
      "Epoch 1/20\n",
      "9084/9084 [==============================] - 0s 31us/step - loss: 2.0717 - acc: 0.3108 - val_loss: 2.0567 - val_acc: 0.3277\n",
      "Epoch 2/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 2.0087 - acc: 0.3275 - val_loss: 1.9221 - val_acc: 0.3277\n",
      "Epoch 3/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.8525 - acc: 0.3275 - val_loss: 1.8207 - val_acc: 0.3277\n",
      "Epoch 4/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.8004 - acc: 0.3275 - val_loss: 1.7926 - val_acc: 0.3277\n",
      "Epoch 5/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7862 - acc: 0.3275 - val_loss: 1.7829 - val_acc: 0.3277\n",
      "Epoch 6/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7788 - acc: 0.3275 - val_loss: 1.7758 - val_acc: 0.3277\n",
      "Epoch 7/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7714 - acc: 0.3275 - val_loss: 1.7683 - val_acc: 0.3277\n",
      "Epoch 8/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7625 - acc: 0.3275 - val_loss: 1.7600 - val_acc: 0.3277\n",
      "Epoch 9/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7530 - acc: 0.3275 - val_loss: 1.7508 - val_acc: 0.3277\n",
      "Epoch 10/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7431 - acc: 0.3290 - val_loss: 1.7428 - val_acc: 0.3307\n",
      "Epoch 11/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7337 - acc: 0.3326 - val_loss: 1.7354 - val_acc: 0.3416\n",
      "Epoch 12/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7251 - acc: 0.3421 - val_loss: 1.7265 - val_acc: 0.3485\n",
      "Epoch 13/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.7142 - acc: 0.3544 - val_loss: 1.7179 - val_acc: 0.3634\n",
      "Epoch 14/20\n",
      "9084/9084 [==============================] - 0s 8us/step - loss: 1.6997 - acc: 0.3702 - val_loss: 1.7025 - val_acc: 0.3832\n",
      "Epoch 15/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.6831 - acc: 0.3866 - val_loss: 1.6903 - val_acc: 0.3881\n",
      "Epoch 16/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.6710 - acc: 0.3939 - val_loss: 1.6815 - val_acc: 0.3901\n",
      "Epoch 17/20\n",
      "9084/9084 [==============================] - 0s 8us/step - loss: 1.6642 - acc: 0.3949 - val_loss: 1.6773 - val_acc: 0.3901\n",
      "Epoch 18/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.6616 - acc: 0.3920 - val_loss: 1.6748 - val_acc: 0.3970\n",
      "Epoch 19/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.6573 - acc: 0.3934 - val_loss: 1.6717 - val_acc: 0.3901\n",
      "Epoch 20/20\n",
      "9084/9084 [==============================] - 0s 7us/step - loss: 1.6566 - acc: 0.3928 - val_loss: 1.6750 - val_acc: 0.3931\n",
      "8908/8908 [==============================] - 0s 15us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       925\n",
      "           2       0.00      0.00      0.00       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.00      0.00      0.00       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.23      0.47      0.31      1625\n",
      "           7       0.00      0.00      0.00      1247\n",
      "           8       0.48      0.91      0.62      2959\n",
      "\n",
      "   micro avg       0.39      0.39      0.39      8908\n",
      "   macro avg       0.09      0.17      0.12      8908\n",
      "weighted avg       0.20      0.39      0.26      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.23      0.24      0.24       925\n",
      "           2       0.20      0.20      0.20       975\n",
      "           3       0.40      0.14      0.21       150\n",
      "           4       0.48      0.48      0.48       199\n",
      "           5       0.51      0.25      0.33       828\n",
      "           6       0.32      0.31      0.32      1625\n",
      "           7       0.33      0.21      0.26      1247\n",
      "           8       0.60      0.81      0.69      2959\n",
      "\n",
      "   micro avg       0.44      0.44      0.44      8908\n",
      "   macro avg       0.38      0.33      0.34      8908\n",
      "weighted avg       0.42      0.44      0.42      8908\n",
      "\n",
      "data sample 30.0\n",
      "Train on 13626 samples, validate on 1515 samples\n",
      "Epoch 1/20\n",
      "13626/13626 [==============================] - 1s 73us/step - loss: 0.5305 - val_loss: 0.3339\n",
      "Epoch 2/20\n",
      "13626/13626 [==============================] - 0s 13us/step - loss: 0.3016 - val_loss: 0.2948\n",
      "Epoch 3/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2895 - val_loss: 0.2902\n",
      "Epoch 4/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2863 - val_loss: 0.2884\n",
      "Epoch 5/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2851 - val_loss: 0.2875\n",
      "Epoch 6/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2842 - val_loss: 0.2867\n",
      "Epoch 7/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2831 - val_loss: 0.2854\n",
      "Epoch 8/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2790 - val_loss: 0.2760\n",
      "Epoch 9/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2670 - val_loss: 0.2658\n",
      "Epoch 10/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2598 - val_loss: 0.2598\n",
      "Epoch 11/20\n",
      "13626/13626 [==============================] - 0s 13us/step - loss: 0.2543 - val_loss: 0.2549\n",
      "Epoch 12/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2503 - val_loss: 0.2517\n",
      "Epoch 13/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2460 - val_loss: 0.2457\n",
      "Epoch 14/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2398 - val_loss: 0.2409\n",
      "Epoch 15/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2362 - val_loss: 0.2382\n",
      "Epoch 16/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2339 - val_loss: 0.2366\n",
      "Epoch 17/20\n",
      "13626/13626 [==============================] - 0s 13us/step - loss: 0.2324 - val_loss: 0.2355\n",
      "Epoch 18/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2312 - val_loss: 0.2342\n",
      "Epoch 19/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2298 - val_loss: 0.2330\n",
      "Epoch 20/20\n",
      "13626/13626 [==============================] - 0s 12us/step - loss: 0.2281 - val_loss: 0.2305\n",
      "Train on 13626 samples, validate on 1515 samples\n",
      "Epoch 1/20\n",
      "13626/13626 [==============================] - 0s 26us/step - loss: 2.0588 - acc: 0.1876 - val_loss: 2.0017 - val_acc: 0.1901\n",
      "Epoch 2/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.8932 - acc: 0.2757 - val_loss: 1.8271 - val_acc: 0.3274\n",
      "Epoch 3/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.8099 - acc: 0.3275 - val_loss: 1.8047 - val_acc: 0.3274\n",
      "Epoch 4/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7958 - acc: 0.3275 - val_loss: 1.7943 - val_acc: 0.3274\n",
      "Epoch 5/20\n",
      "13626/13626 [==============================] - 0s 8us/step - loss: 1.7843 - acc: 0.3275 - val_loss: 1.7795 - val_acc: 0.3274\n",
      "Epoch 6/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7673 - acc: 0.3275 - val_loss: 1.7588 - val_acc: 0.3274\n",
      "Epoch 7/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7448 - acc: 0.3275 - val_loss: 1.7317 - val_acc: 0.3274\n",
      "Epoch 8/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7259 - acc: 0.3281 - val_loss: 1.7162 - val_acc: 0.3300\n",
      "Epoch 9/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7179 - acc: 0.3319 - val_loss: 1.7111 - val_acc: 0.3340\n",
      "Epoch 10/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7138 - acc: 0.3374 - val_loss: 1.7061 - val_acc: 0.3380\n",
      "Epoch 11/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7101 - acc: 0.3402 - val_loss: 1.7042 - val_acc: 0.3492\n",
      "Epoch 12/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7073 - acc: 0.3418 - val_loss: 1.7015 - val_acc: 0.3426\n",
      "Epoch 13/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7053 - acc: 0.3460 - val_loss: 1.6974 - val_acc: 0.3538\n",
      "Epoch 14/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7029 - acc: 0.3496 - val_loss: 1.6958 - val_acc: 0.3525\n",
      "Epoch 15/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.7012 - acc: 0.3533 - val_loss: 1.6935 - val_acc: 0.3558\n",
      "Epoch 16/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.6998 - acc: 0.3562 - val_loss: 1.6908 - val_acc: 0.3696\n",
      "Epoch 17/20\n",
      "13626/13626 [==============================] - 0s 8us/step - loss: 1.6980 - acc: 0.3635 - val_loss: 1.6886 - val_acc: 0.3729\n",
      "Epoch 18/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.6965 - acc: 0.3658 - val_loss: 1.6904 - val_acc: 0.3683\n",
      "Epoch 19/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.6959 - acc: 0.3663 - val_loss: 1.6865 - val_acc: 0.3835\n",
      "Epoch 20/20\n",
      "13626/13626 [==============================] - 0s 7us/step - loss: 1.6944 - acc: 0.3729 - val_loss: 1.6856 - val_acc: 0.3769\n",
      "8908/8908 [==============================] - 0s 23us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       925\n",
      "           2       0.00      0.00      0.00       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.00      0.00      0.00       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.25      0.32      0.28      1625\n",
      "           7       0.00      0.00      0.00      1247\n",
      "           8       0.40      0.93      0.56      2959\n",
      "\n",
      "   micro avg       0.37      0.37      0.37      8908\n",
      "   macro avg       0.08      0.16      0.11      8908\n",
      "weighted avg       0.18      0.37      0.24      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.26      0.26       925\n",
      "           2       0.24      0.20      0.22       975\n",
      "           3       0.39      0.16      0.23       150\n",
      "           4       0.41      0.35      0.38       199\n",
      "           5       0.55      0.26      0.35       828\n",
      "           6       0.33      0.40      0.36      1625\n",
      "           7       0.32      0.26      0.29      1247\n",
      "           8       0.62      0.74      0.67      2959\n",
      "\n",
      "   micro avg       0.44      0.44      0.44      8908\n",
      "   macro avg       0.39      0.33      0.34      8908\n",
      "weighted avg       0.43      0.44      0.42      8908\n",
      "\n",
      "data sample 40.0\n",
      "Train on 18170 samples, validate on 2019 samples\n",
      "Epoch 1/20\n",
      "18170/18170 [==============================] - 1s 69us/step - loss: 0.5248 - val_loss: 0.3040\n",
      "Epoch 2/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2969 - val_loss: 0.2891\n",
      "Epoch 3/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2879 - val_loss: 0.2842\n",
      "Epoch 4/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2852 - val_loss: 0.2829\n",
      "Epoch 5/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2834 - val_loss: 0.2792\n",
      "Epoch 6/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2753 - val_loss: 0.2669\n",
      "Epoch 7/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2651 - val_loss: 0.2601\n",
      "Epoch 8/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2604 - val_loss: 0.2563\n",
      "Epoch 9/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2570 - val_loss: 0.2537\n",
      "Epoch 10/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2547 - val_loss: 0.2519\n",
      "Epoch 11/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2531 - val_loss: 0.2504\n",
      "Epoch 12/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2513 - val_loss: 0.2481\n",
      "Epoch 13/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2484 - val_loss: 0.2445\n",
      "Epoch 14/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2441 - val_loss: 0.2405\n",
      "Epoch 15/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2405 - val_loss: 0.2380\n",
      "Epoch 16/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2381 - val_loss: 0.2360\n",
      "Epoch 17/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2360 - val_loss: 0.2338\n",
      "Epoch 18/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2341 - val_loss: 0.2324\n",
      "Epoch 19/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2322 - val_loss: 0.2304\n",
      "Epoch 20/20\n",
      "18170/18170 [==============================] - 0s 13us/step - loss: 0.2304 - val_loss: 0.2289\n",
      "Train on 18170 samples, validate on 2019 samples\n",
      "Epoch 1/20\n",
      "18170/18170 [==============================] - 0s 24us/step - loss: 2.0617 - acc: 0.3189 - val_loss: 2.0173 - val_acc: 0.3274\n",
      "Epoch 2/20\n",
      "18170/18170 [==============================] - 0s 8us/step - loss: 1.8914 - acc: 0.3275 - val_loss: 1.8148 - val_acc: 0.3274\n",
      "Epoch 3/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7902 - acc: 0.3275 - val_loss: 1.7813 - val_acc: 0.3274\n",
      "Epoch 4/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7751 - acc: 0.3275 - val_loss: 1.7695 - val_acc: 0.3274\n",
      "Epoch 5/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7662 - acc: 0.3275 - val_loss: 1.7590 - val_acc: 0.3274\n",
      "Epoch 6/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7585 - acc: 0.3280 - val_loss: 1.7515 - val_acc: 0.3284\n",
      "Epoch 7/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7523 - acc: 0.3308 - val_loss: 1.7473 - val_acc: 0.3269\n",
      "Epoch 8/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7477 - acc: 0.3319 - val_loss: 1.7431 - val_acc: 0.3314\n",
      "Epoch 9/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7438 - acc: 0.3365 - val_loss: 1.7385 - val_acc: 0.3358\n",
      "Epoch 10/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7364 - acc: 0.3453 - val_loss: 1.7227 - val_acc: 0.3616\n",
      "Epoch 11/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7232 - acc: 0.3665 - val_loss: 1.7174 - val_acc: 0.3675\n",
      "Epoch 12/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7167 - acc: 0.3723 - val_loss: 1.7143 - val_acc: 0.3680\n",
      "Epoch 13/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7122 - acc: 0.3729 - val_loss: 1.7084 - val_acc: 0.3690\n",
      "Epoch 14/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7067 - acc: 0.3730 - val_loss: 1.7009 - val_acc: 0.3695\n",
      "Epoch 15/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.7019 - acc: 0.3719 - val_loss: 1.6956 - val_acc: 0.3660\n",
      "Epoch 16/20\n",
      "18170/18170 [==============================] - 0s 8us/step - loss: 1.6954 - acc: 0.3724 - val_loss: 1.6884 - val_acc: 0.3720\n",
      "Epoch 17/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.6904 - acc: 0.3755 - val_loss: 1.6831 - val_acc: 0.3794\n",
      "Epoch 18/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.6873 - acc: 0.3748 - val_loss: 1.6783 - val_acc: 0.3789\n",
      "Epoch 19/20\n",
      "18170/18170 [==============================] - 0s 8us/step - loss: 1.6829 - acc: 0.3781 - val_loss: 1.6756 - val_acc: 0.3749\n",
      "Epoch 20/20\n",
      "18170/18170 [==============================] - 0s 7us/step - loss: 1.6787 - acc: 0.3790 - val_loss: 1.6736 - val_acc: 0.3754\n",
      "8908/8908 [==============================] - 0s 31us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.08      0.13       925\n",
      "           2       0.00      0.00      0.00       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.00      0.00      0.00       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.21      0.31      0.25      1625\n",
      "           7       0.00      0.00      0.00      1247\n",
      "           8       0.44      0.93      0.60      2959\n",
      "\n",
      "   micro avg       0.37      0.37      0.37      8908\n",
      "   macro avg       0.12      0.17      0.12      8908\n",
      "weighted avg       0.21      0.37      0.26      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.25      0.25       925\n",
      "           2       0.23      0.21      0.22       975\n",
      "           3       0.38      0.26      0.31       150\n",
      "           4       0.50      0.46      0.48       199\n",
      "           5       0.49      0.33      0.39       828\n",
      "           6       0.32      0.32      0.32      1625\n",
      "           7       0.31      0.23      0.27      1247\n",
      "           8       0.61      0.76      0.68      2959\n",
      "\n",
      "   micro avg       0.44      0.44      0.44      8908\n",
      "   macro avg       0.39      0.35      0.36      8908\n",
      "weighted avg       0.42      0.44      0.42      8908\n",
      "\n",
      "data sample 50.0\n",
      "Train on 22712 samples, validate on 2524 samples\n",
      "Epoch 1/20\n",
      "22712/22712 [==============================] - 2s 66us/step - loss: 0.4400 - val_loss: 0.2956\n",
      "Epoch 2/20\n",
      "22712/22712 [==============================] - 0s 16us/step - loss: 0.2897 - val_loss: 0.2865\n",
      "Epoch 3/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2852 - val_loss: 0.2846\n",
      "Epoch 4/20\n",
      "22712/22712 [==============================] - 0s 16us/step - loss: 0.2831 - val_loss: 0.2809\n",
      "Epoch 5/20\n",
      "22712/22712 [==============================] - 0s 16us/step - loss: 0.2723 - val_loss: 0.2649\n",
      "Epoch 6/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2609 - val_loss: 0.2568\n",
      "Epoch 7/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2515 - val_loss: 0.2468\n",
      "Epoch 8/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2433 - val_loss: 0.2411\n",
      "Epoch 9/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2380 - val_loss: 0.2366\n",
      "Epoch 10/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2341 - val_loss: 0.2334\n",
      "Epoch 11/20\n",
      "22712/22712 [==============================] - 0s 16us/step - loss: 0.2316 - val_loss: 0.2312\n",
      "Epoch 12/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2297 - val_loss: 0.2294\n",
      "Epoch 13/20\n",
      "22712/22712 [==============================] - 0s 16us/step - loss: 0.2277 - val_loss: 0.2265\n",
      "Epoch 14/20\n",
      "22712/22712 [==============================] - 0s 16us/step - loss: 0.2244 - val_loss: 0.2232\n",
      "Epoch 15/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2218 - val_loss: 0.2212\n",
      "Epoch 16/20\n",
      "22712/22712 [==============================] - 0s 15us/step - loss: 0.2198 - val_loss: 0.2191\n",
      "Epoch 17/20\n",
      "22712/22712 [==============================] - 0s 13us/step - loss: 0.2176 - val_loss: 0.2169\n",
      "Epoch 18/20\n",
      "22712/22712 [==============================] - 0s 13us/step - loss: 0.2156 - val_loss: 0.2150\n",
      "Epoch 19/20\n",
      "22712/22712 [==============================] - 0s 13us/step - loss: 0.2140 - val_loss: 0.2136\n",
      "Epoch 20/20\n",
      "22712/22712 [==============================] - 0s 13us/step - loss: 0.2126 - val_loss: 0.2127\n",
      "Train on 22712 samples, validate on 2524 samples\n",
      "Epoch 1/20\n",
      "22712/22712 [==============================] - 1s 23us/step - loss: 2.0114 - acc: 0.3208 - val_loss: 1.8345 - val_acc: 0.3277\n",
      "Epoch 2/20\n",
      "22712/22712 [==============================] - 0s 8us/step - loss: 1.8036 - acc: 0.3275 - val_loss: 1.7827 - val_acc: 0.3277\n",
      "Epoch 3/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.7733 - acc: 0.3275 - val_loss: 1.7600 - val_acc: 0.3277\n",
      "Epoch 4/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.7416 - acc: 0.3285 - val_loss: 1.7273 - val_acc: 0.3391\n",
      "Epoch 5/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.7006 - acc: 0.3683 - val_loss: 1.6866 - val_acc: 0.3796\n",
      "Epoch 6/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6714 - acc: 0.3896 - val_loss: 1.6756 - val_acc: 0.3831\n",
      "Epoch 7/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6650 - acc: 0.3883 - val_loss: 1.6718 - val_acc: 0.3768\n",
      "Epoch 8/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6602 - acc: 0.3874 - val_loss: 1.6693 - val_acc: 0.3784\n",
      "Epoch 9/20\n",
      "22712/22712 [==============================] - 0s 8us/step - loss: 1.6577 - acc: 0.3870 - val_loss: 1.6655 - val_acc: 0.3712\n",
      "Epoch 10/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6558 - acc: 0.3870 - val_loss: 1.6605 - val_acc: 0.3732\n",
      "Epoch 11/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6532 - acc: 0.3878 - val_loss: 1.6574 - val_acc: 0.3780\n",
      "Epoch 12/20\n",
      "22712/22712 [==============================] - 0s 8us/step - loss: 1.6519 - acc: 0.3872 - val_loss: 1.6575 - val_acc: 0.3764\n",
      "Epoch 13/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6491 - acc: 0.3869 - val_loss: 1.6533 - val_acc: 0.3764\n",
      "Epoch 14/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6465 - acc: 0.3873 - val_loss: 1.6519 - val_acc: 0.3780\n",
      "Epoch 15/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6442 - acc: 0.3865 - val_loss: 1.6488 - val_acc: 0.3772\n",
      "Epoch 16/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6420 - acc: 0.3877 - val_loss: 1.6455 - val_acc: 0.3800\n",
      "Epoch 17/20\n",
      "22712/22712 [==============================] - 0s 8us/step - loss: 1.6379 - acc: 0.3919 - val_loss: 1.6414 - val_acc: 0.3827\n",
      "Epoch 18/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6377 - acc: 0.3904 - val_loss: 1.6370 - val_acc: 0.3859\n",
      "Epoch 19/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6314 - acc: 0.3936 - val_loss: 1.6336 - val_acc: 0.3942\n",
      "Epoch 20/20\n",
      "22712/22712 [==============================] - 0s 7us/step - loss: 1.6267 - acc: 0.3996 - val_loss: 1.6319 - val_acc: 0.3966\n",
      "8908/8908 [==============================] - 0s 39us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.31      0.06      0.10       925\n",
      "           2       0.31      0.10      0.15       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.00      0.00      0.00       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.24      0.33      0.28      1625\n",
      "           7       0.41      0.08      0.13      1247\n",
      "           8       0.46      0.93      0.62      2959\n",
      "\n",
      "   micro avg       0.40      0.40      0.40      8908\n",
      "   macro avg       0.22      0.19      0.16      8908\n",
      "weighted avg       0.32      0.40      0.30      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.22      0.22      0.22       925\n",
      "           2       0.25      0.24      0.24       975\n",
      "           3       0.38      0.23      0.29       150\n",
      "           4       0.47      0.43      0.45       199\n",
      "           5       0.42      0.26      0.32       828\n",
      "           6       0.33      0.36      0.34      1625\n",
      "           7       0.33      0.25      0.28      1247\n",
      "           8       0.63      0.77      0.69      2959\n",
      "\n",
      "   micro avg       0.44      0.44      0.44      8908\n",
      "   macro avg       0.38      0.34      0.36      8908\n",
      "weighted avg       0.42      0.44      0.43      8908\n",
      "\n",
      "data sample 60.0\n",
      "Train on 27254 samples, validate on 3029 samples\n",
      "Epoch 1/20\n",
      "27254/27254 [==============================] - 2s 65us/step - loss: 0.4424 - val_loss: 0.2928\n",
      "Epoch 2/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2893 - val_loss: 0.2849\n",
      "Epoch 3/20\n",
      "27254/27254 [==============================] - 0s 13us/step - loss: 0.2849 - val_loss: 0.2819\n",
      "Epoch 4/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2750 - val_loss: 0.2646\n",
      "Epoch 5/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2619 - val_loss: 0.2580\n",
      "Epoch 6/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2576 - val_loss: 0.2544\n",
      "Epoch 7/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2526 - val_loss: 0.2473\n",
      "Epoch 8/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2430 - val_loss: 0.2361\n",
      "Epoch 9/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2344 - val_loss: 0.2296\n",
      "Epoch 10/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2292 - val_loss: 0.2259\n",
      "Epoch 11/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2260 - val_loss: 0.2235\n",
      "Epoch 12/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2235 - val_loss: 0.2209\n",
      "Epoch 13/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2212 - val_loss: 0.2187\n",
      "Epoch 14/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2192 - val_loss: 0.2171\n",
      "Epoch 15/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2165 - val_loss: 0.2134\n",
      "Epoch 16/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2133 - val_loss: 0.2106\n",
      "Epoch 17/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2108 - val_loss: 0.2084\n",
      "Epoch 18/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2082 - val_loss: 0.2058\n",
      "Epoch 19/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2059 - val_loss: 0.2043\n",
      "Epoch 20/20\n",
      "27254/27254 [==============================] - 0s 14us/step - loss: 0.2039 - val_loss: 0.2019\n",
      "Train on 27254 samples, validate on 3029 samples\n",
      "Epoch 1/20\n",
      "27254/27254 [==============================] - 1s 24us/step - loss: 1.9492 - acc: 0.3195 - val_loss: 1.7977 - val_acc: 0.3275\n",
      "Epoch 2/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.7790 - acc: 0.3275 - val_loss: 1.7619 - val_acc: 0.3275\n",
      "Epoch 3/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.7422 - acc: 0.3293 - val_loss: 1.7340 - val_acc: 0.3371\n",
      "Epoch 4/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.7162 - acc: 0.3413 - val_loss: 1.7175 - val_acc: 0.3516\n",
      "Epoch 5/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.7002 - acc: 0.3593 - val_loss: 1.7082 - val_acc: 0.3608\n",
      "Epoch 6/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6880 - acc: 0.3752 - val_loss: 1.6987 - val_acc: 0.3816\n",
      "Epoch 7/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6800 - acc: 0.3880 - val_loss: 1.6967 - val_acc: 0.3807\n",
      "Epoch 8/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6728 - acc: 0.3918 - val_loss: 1.6852 - val_acc: 0.3866\n",
      "Epoch 9/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6682 - acc: 0.3940 - val_loss: 1.6800 - val_acc: 0.3866\n",
      "Epoch 10/20\n",
      "27254/27254 [==============================] - 0s 8us/step - loss: 1.6631 - acc: 0.3945 - val_loss: 1.6755 - val_acc: 0.3882\n",
      "Epoch 11/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6580 - acc: 0.3955 - val_loss: 1.6707 - val_acc: 0.3882\n",
      "Epoch 12/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6514 - acc: 0.3959 - val_loss: 1.6646 - val_acc: 0.3886\n",
      "Epoch 13/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6396 - acc: 0.3920 - val_loss: 1.6424 - val_acc: 0.3849\n",
      "Epoch 14/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6164 - acc: 0.3963 - val_loss: 1.6270 - val_acc: 0.3873\n",
      "Epoch 15/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6100 - acc: 0.4001 - val_loss: 1.6262 - val_acc: 0.3889\n",
      "Epoch 16/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6076 - acc: 0.4031 - val_loss: 1.6213 - val_acc: 0.3972\n",
      "Epoch 17/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6050 - acc: 0.4046 - val_loss: 1.6197 - val_acc: 0.3955\n",
      "Epoch 18/20\n",
      "27254/27254 [==============================] - 0s 10us/step - loss: 1.6051 - acc: 0.4053 - val_loss: 1.6237 - val_acc: 0.3962\n",
      "Epoch 19/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6039 - acc: 0.4045 - val_loss: 1.6163 - val_acc: 0.3991\n",
      "Epoch 20/20\n",
      "27254/27254 [==============================] - 0s 9us/step - loss: 1.6016 - acc: 0.4068 - val_loss: 1.6178 - val_acc: 0.4001\n",
      "8908/8908 [==============================] - 0s 52us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.34      0.15      0.20       925\n",
      "           2       0.33      0.07      0.12       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.22      0.25      0.23       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.27      0.51      0.36      1625\n",
      "           7       0.00      0.00      0.00      1247\n",
      "           8       0.52      0.88      0.65      2959\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      8908\n",
      "   macro avg       0.21      0.23      0.20      8908\n",
      "weighted avg       0.30      0.41      0.32      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.23      0.26      0.25       925\n",
      "           2       0.25      0.21      0.23       975\n",
      "           3       0.33      0.23      0.27       150\n",
      "           4       0.46      0.32      0.38       199\n",
      "           5       0.50      0.26      0.34       828\n",
      "           6       0.33      0.33      0.33      1625\n",
      "           7       0.30      0.21      0.25      1247\n",
      "           8       0.61      0.80      0.69      2959\n",
      "\n",
      "   micro avg       0.44      0.44      0.44      8908\n",
      "   macro avg       0.38      0.33      0.34      8908\n",
      "weighted avg       0.42      0.44      0.42      8908\n",
      "\n",
      "data sample 70.0\n",
      "Train on 31797 samples, validate on 3534 samples\n",
      "Epoch 1/20\n",
      "31797/31797 [==============================] - 2s 65us/step - loss: 0.3939 - val_loss: 0.2859\n",
      "Epoch 2/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2846 - val_loss: 0.2825\n",
      "Epoch 3/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2799 - val_loss: 0.2705\n",
      "Epoch 4/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2606 - val_loss: 0.2525\n",
      "Epoch 5/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2476 - val_loss: 0.2414\n",
      "Epoch 6/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2378 - val_loss: 0.2342\n",
      "Epoch 7/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2322 - val_loss: 0.2288\n",
      "Epoch 8/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2271 - val_loss: 0.2247\n",
      "Epoch 9/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2236 - val_loss: 0.2217\n",
      "Epoch 10/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2213 - val_loss: 0.2200\n",
      "Epoch 11/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2192 - val_loss: 0.2178\n",
      "Epoch 12/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2159 - val_loss: 0.2143\n",
      "Epoch 13/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2136 - val_loss: 0.2128\n",
      "Epoch 14/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2122 - val_loss: 0.2116\n",
      "Epoch 15/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2110 - val_loss: 0.2103\n",
      "Epoch 16/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2099 - val_loss: 0.2102\n",
      "Epoch 17/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2088 - val_loss: 0.2090\n",
      "Epoch 18/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2077 - val_loss: 0.2071\n",
      "Epoch 19/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2061 - val_loss: 0.2057\n",
      "Epoch 20/20\n",
      "31797/31797 [==============================] - 0s 14us/step - loss: 0.2045 - val_loss: 0.2041\n",
      "Train on 31797 samples, validate on 3534 samples\n",
      "Epoch 1/20\n",
      "31797/31797 [==============================] - 1s 23us/step - loss: 1.9450 - acc: 0.2319 - val_loss: 1.7789 - val_acc: 0.3274\n",
      "Epoch 2/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.7716 - acc: 0.3275 - val_loss: 1.7703 - val_acc: 0.3274\n",
      "Epoch 3/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.7626 - acc: 0.3275 - val_loss: 1.7609 - val_acc: 0.3274\n",
      "Epoch 4/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.7526 - acc: 0.3277 - val_loss: 1.7530 - val_acc: 0.3299\n",
      "Epoch 5/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.7419 - acc: 0.3382 - val_loss: 1.7324 - val_acc: 0.3563\n",
      "Epoch 6/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.7099 - acc: 0.3654 - val_loss: 1.7112 - val_acc: 0.3738\n",
      "Epoch 7/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6930 - acc: 0.3727 - val_loss: 1.6978 - val_acc: 0.3724\n",
      "Epoch 8/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6844 - acc: 0.3755 - val_loss: 1.6927 - val_acc: 0.3746\n",
      "Epoch 9/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6789 - acc: 0.3762 - val_loss: 1.6888 - val_acc: 0.3721\n",
      "Epoch 10/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6726 - acc: 0.3776 - val_loss: 1.6815 - val_acc: 0.3746\n",
      "Epoch 11/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6667 - acc: 0.3781 - val_loss: 1.6825 - val_acc: 0.3789\n",
      "Epoch 12/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6607 - acc: 0.3809 - val_loss: 1.6705 - val_acc: 0.3783\n",
      "Epoch 13/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6553 - acc: 0.3834 - val_loss: 1.6659 - val_acc: 0.3783\n",
      "Epoch 14/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6510 - acc: 0.3838 - val_loss: 1.6615 - val_acc: 0.3809\n",
      "Epoch 15/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6461 - acc: 0.3903 - val_loss: 1.6630 - val_acc: 0.3848\n",
      "Epoch 16/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6430 - acc: 0.3935 - val_loss: 1.6556 - val_acc: 0.3950\n",
      "Epoch 17/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6390 - acc: 0.3974 - val_loss: 1.6546 - val_acc: 0.3990\n",
      "Epoch 18/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6374 - acc: 0.3979 - val_loss: 1.6536 - val_acc: 0.4007\n",
      "Epoch 19/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6336 - acc: 0.4010 - val_loss: 1.6503 - val_acc: 0.4029\n",
      "Epoch 20/20\n",
      "31797/31797 [==============================] - 0s 8us/step - loss: 1.6318 - acc: 0.4030 - val_loss: 1.6491 - val_acc: 0.4035\n",
      "8908/8908 [==============================] - 1s 57us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.13      0.20       925\n",
      "           2       0.28      0.05      0.09       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.00      0.00      0.00       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.26      0.48      0.34      1625\n",
      "           7       0.39      0.11      0.18      1247\n",
      "           8       0.50      0.87      0.64      2959\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      8908\n",
      "   macro avg       0.23      0.21      0.18      8908\n",
      "weighted avg       0.34      0.41      0.33      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.24      0.26      0.25       925\n",
      "           2       0.23      0.22      0.23       975\n",
      "           3       0.32      0.24      0.27       150\n",
      "           4       0.52      0.36      0.43       199\n",
      "           5       0.47      0.31      0.37       828\n",
      "           6       0.34      0.33      0.34      1625\n",
      "           7       0.32      0.23      0.27      1247\n",
      "           8       0.62      0.76      0.68      2959\n",
      "\n",
      "   micro avg       0.44      0.44      0.44      8908\n",
      "   macro avg       0.38      0.34      0.35      8908\n",
      "weighted avg       0.42      0.44      0.43      8908\n",
      "\n",
      "data sample 80.0\n",
      "Train on 36340 samples, validate on 4038 samples\n",
      "Epoch 1/20\n",
      "36340/36340 [==============================] - 2s 65us/step - loss: 0.3916 - val_loss: 0.2886\n",
      "Epoch 2/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2863 - val_loss: 0.2834\n",
      "Epoch 3/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2826 - val_loss: 0.2756\n",
      "Epoch 4/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2643 - val_loss: 0.2550\n",
      "Epoch 5/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2505 - val_loss: 0.2455\n",
      "Epoch 6/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2434 - val_loss: 0.2395\n",
      "Epoch 7/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2377 - val_loss: 0.2340\n",
      "Epoch 8/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2325 - val_loss: 0.2288\n",
      "Epoch 9/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2267 - val_loss: 0.2229\n",
      "Epoch 10/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2220 - val_loss: 0.2191\n",
      "Epoch 11/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2187 - val_loss: 0.2162\n",
      "Epoch 12/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2158 - val_loss: 0.2136\n",
      "Epoch 13/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2135 - val_loss: 0.2113\n",
      "Epoch 14/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2110 - val_loss: 0.2089\n",
      "Epoch 15/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2085 - val_loss: 0.2067\n",
      "Epoch 16/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2063 - val_loss: 0.2047\n",
      "Epoch 17/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2046 - val_loss: 0.2031\n",
      "Epoch 18/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2032 - val_loss: 0.2021\n",
      "Epoch 19/20\n",
      "36340/36340 [==============================] - 1s 14us/step - loss: 0.2018 - val_loss: 0.2012\n",
      "Epoch 20/20\n",
      "36340/36340 [==============================] - 1s 15us/step - loss: 0.2007 - val_loss: 0.1996\n",
      "Train on 36340 samples, validate on 4038 samples\n",
      "Epoch 1/20\n",
      "36340/36340 [==============================] - 1s 27us/step - loss: 1.9088 - acc: 0.3247 - val_loss: 1.7850 - val_acc: 0.3276\n",
      "Epoch 2/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.7774 - acc: 0.3275 - val_loss: 1.7572 - val_acc: 0.3276\n",
      "Epoch 3/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.7411 - acc: 0.3280 - val_loss: 1.7193 - val_acc: 0.3370\n",
      "Epoch 4/20\n",
      "36340/36340 [==============================] - 0s 9us/step - loss: 1.6969 - acc: 0.3640 - val_loss: 1.6701 - val_acc: 0.3856\n",
      "Epoch 5/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.6710 - acc: 0.3840 - val_loss: 1.6585 - val_acc: 0.3848\n",
      "Epoch 6/20\n",
      "36340/36340 [==============================] - 0s 9us/step - loss: 1.6590 - acc: 0.3831 - val_loss: 1.6490 - val_acc: 0.3787\n",
      "Epoch 7/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.6459 - acc: 0.3852 - val_loss: 1.6371 - val_acc: 0.3843\n",
      "Epoch 8/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.6359 - acc: 0.3859 - val_loss: 1.6286 - val_acc: 0.3853\n",
      "Epoch 9/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.6283 - acc: 0.3876 - val_loss: 1.6219 - val_acc: 0.3930\n",
      "Epoch 10/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.6241 - acc: 0.3903 - val_loss: 1.6196 - val_acc: 0.3908\n",
      "Epoch 11/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.6196 - acc: 0.3906 - val_loss: 1.6159 - val_acc: 0.3950\n",
      "Epoch 12/20\n",
      "36340/36340 [==============================] - 0s 10us/step - loss: 1.6164 - acc: 0.3939 - val_loss: 1.6154 - val_acc: 0.3903\n",
      "Epoch 13/20\n",
      "36340/36340 [==============================] - 0s 9us/step - loss: 1.6144 - acc: 0.3944 - val_loss: 1.6132 - val_acc: 0.3965\n",
      "Epoch 14/20\n",
      "36340/36340 [==============================] - 0s 9us/step - loss: 1.6120 - acc: 0.3971 - val_loss: 1.6089 - val_acc: 0.3990\n",
      "Epoch 15/20\n",
      "36340/36340 [==============================] - 0s 8us/step - loss: 1.6092 - acc: 0.3968 - val_loss: 1.6063 - val_acc: 0.3985\n",
      "Epoch 16/20\n",
      "36340/36340 [==============================] - 0s 8us/step - loss: 1.6080 - acc: 0.3987 - val_loss: 1.6056 - val_acc: 0.3975\n",
      "Epoch 17/20\n",
      "36340/36340 [==============================] - 0s 8us/step - loss: 1.6064 - acc: 0.3972 - val_loss: 1.6068 - val_acc: 0.3980\n",
      "Epoch 18/20\n",
      "36340/36340 [==============================] - 0s 8us/step - loss: 1.6061 - acc: 0.3986 - val_loss: 1.6108 - val_acc: 0.3950\n",
      "Epoch 19/20\n",
      "36340/36340 [==============================] - 0s 8us/step - loss: 1.6052 - acc: 0.3991 - val_loss: 1.6044 - val_acc: 0.3920\n",
      "Epoch 20/20\n",
      "36340/36340 [==============================] - 0s 8us/step - loss: 1.6032 - acc: 0.3992 - val_loss: 1.6062 - val_acc: 0.4000\n",
      "8908/8908 [==============================] - 1s 66us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.37      0.13      0.20       925\n",
      "           2       0.24      0.07      0.11       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.00      0.00      0.00       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.26      0.50      0.35      1625\n",
      "           7       0.38      0.11      0.17      1247\n",
      "           8       0.53      0.86      0.65      2959\n",
      "\n",
      "   micro avg       0.42      0.42      0.42      8908\n",
      "   macro avg       0.22      0.21      0.18      8908\n",
      "weighted avg       0.34      0.42      0.34      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.23      0.23      0.23       925\n",
      "           2       0.24      0.22      0.23       975\n",
      "           3       0.37      0.30      0.33       150\n",
      "           4       0.51      0.38      0.44       199\n",
      "           5       0.42      0.32      0.36       828\n",
      "           6       0.31      0.33      0.32      1625\n",
      "           7       0.28      0.20      0.23      1247\n",
      "           8       0.62      0.75      0.68      2959\n",
      "\n",
      "   micro avg       0.43      0.43      0.43      8908\n",
      "   macro avg       0.37      0.34      0.35      8908\n",
      "weighted avg       0.41      0.43      0.42      8908\n",
      "\n",
      "data sample 90.0\n",
      "Train on 40882 samples, validate on 4543 samples\n",
      "Epoch 1/20\n",
      "40882/40882 [==============================] - 3s 67us/step - loss: 0.3769 - val_loss: 0.2815\n",
      "Epoch 2/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2703 - val_loss: 0.2610\n",
      "Epoch 3/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2591 - val_loss: 0.2531\n",
      "Epoch 4/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2490 - val_loss: 0.2432\n",
      "Epoch 5/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2419 - val_loss: 0.2364\n",
      "Epoch 6/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2347 - val_loss: 0.2305\n",
      "Epoch 7/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2294 - val_loss: 0.2259\n",
      "Epoch 8/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2254 - val_loss: 0.2223\n",
      "Epoch 9/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2214 - val_loss: 0.2179\n",
      "Epoch 10/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2169 - val_loss: 0.2136\n",
      "Epoch 11/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2134 - val_loss: 0.2113\n",
      "Epoch 12/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2112 - val_loss: 0.2094\n",
      "Epoch 13/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2095 - val_loss: 0.2083\n",
      "Epoch 14/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2082 - val_loss: 0.2071\n",
      "Epoch 15/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2070 - val_loss: 0.2057\n",
      "Epoch 16/20\n",
      "40882/40882 [==============================] - 1s 15us/step - loss: 0.2059 - val_loss: 0.2046\n",
      "Epoch 17/20\n",
      "40882/40882 [==============================] - 1s 16us/step - loss: 0.2049 - val_loss: 0.2037\n",
      "Epoch 18/20\n",
      "40882/40882 [==============================] - 1s 17us/step - loss: 0.2040 - val_loss: 0.2031\n",
      "Epoch 19/20\n",
      "40882/40882 [==============================] - 1s 17us/step - loss: 0.2032 - val_loss: 0.2022\n",
      "Epoch 20/20\n",
      "40882/40882 [==============================] - 1s 17us/step - loss: 0.2024 - val_loss: 0.2012\n",
      "Train on 40882 samples, validate on 4543 samples\n",
      "Epoch 1/20\n",
      "40882/40882 [==============================] - 1s 24us/step - loss: 1.9060 - acc: 0.3211 - val_loss: 1.7935 - val_acc: 0.3275\n",
      "Epoch 2/20\n",
      "40882/40882 [==============================] - 0s 9us/step - loss: 1.7887 - acc: 0.3275 - val_loss: 1.7770 - val_acc: 0.3275\n",
      "Epoch 3/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.7665 - acc: 0.3275 - val_loss: 1.7527 - val_acc: 0.3278\n",
      "Epoch 4/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.7457 - acc: 0.3318 - val_loss: 1.7400 - val_acc: 0.3348\n",
      "Epoch 5/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.7311 - acc: 0.3475 - val_loss: 1.7183 - val_acc: 0.3689\n",
      "Epoch 6/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.7022 - acc: 0.3696 - val_loss: 1.7022 - val_acc: 0.3632\n",
      "Epoch 7/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6815 - acc: 0.3716 - val_loss: 1.6821 - val_acc: 0.3696\n",
      "Epoch 8/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6714 - acc: 0.3746 - val_loss: 1.6723 - val_acc: 0.3742\n",
      "Epoch 9/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6637 - acc: 0.3788 - val_loss: 1.6638 - val_acc: 0.3762\n",
      "Epoch 10/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6539 - acc: 0.3798 - val_loss: 1.6554 - val_acc: 0.3821\n",
      "Epoch 11/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6486 - acc: 0.3849 - val_loss: 1.6509 - val_acc: 0.3837\n",
      "Epoch 12/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6455 - acc: 0.3864 - val_loss: 1.6505 - val_acc: 0.3905\n",
      "Epoch 13/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6458 - acc: 0.3844 - val_loss: 1.6518 - val_acc: 0.3923\n",
      "Epoch 14/20\n",
      "40882/40882 [==============================] - 0s 9us/step - loss: 1.6421 - acc: 0.3869 - val_loss: 1.6434 - val_acc: 0.3898\n",
      "Epoch 15/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6390 - acc: 0.3873 - val_loss: 1.6439 - val_acc: 0.3883\n",
      "Epoch 16/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6357 - acc: 0.3895 - val_loss: 1.6386 - val_acc: 0.3916\n",
      "Epoch 17/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6342 - acc: 0.3910 - val_loss: 1.6347 - val_acc: 0.3914\n",
      "Epoch 18/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6303 - acc: 0.3906 - val_loss: 1.6387 - val_acc: 0.3960\n",
      "Epoch 19/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6276 - acc: 0.3925 - val_loss: 1.6286 - val_acc: 0.4013\n",
      "Epoch 20/20\n",
      "40882/40882 [==============================] - 0s 8us/step - loss: 1.6249 - acc: 0.3937 - val_loss: 1.6295 - val_acc: 0.4037\n",
      "8908/8908 [==============================] - 1s 77us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.32      0.20      0.25       925\n",
      "           2       0.19      0.01      0.02       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.33      0.03      0.06       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.26      0.43      0.33      1625\n",
      "           7       0.37      0.12      0.18      1247\n",
      "           8       0.49      0.86      0.62      2959\n",
      "\n",
      "   micro avg       0.40      0.40      0.40      8908\n",
      "   macro avg       0.24      0.21      0.18      8908\n",
      "weighted avg       0.32      0.40      0.32      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.24      0.25      0.25       925\n",
      "           2       0.25      0.21      0.23       975\n",
      "           3       0.37      0.19      0.25       150\n",
      "           4       0.53      0.51      0.52       199\n",
      "           5       0.47      0.28      0.35       828\n",
      "           6       0.34      0.35      0.35      1625\n",
      "           7       0.35      0.29      0.32      1247\n",
      "           8       0.63      0.78      0.70      2959\n",
      "\n",
      "   micro avg       0.45      0.45      0.45      8908\n",
      "   macro avg       0.40      0.36      0.37      8908\n",
      "weighted avg       0.44      0.45      0.44      8908\n",
      "\n",
      "data sample 99.0\n",
      "Train on 44971 samples, validate on 4997 samples\n",
      "Epoch 1/20\n",
      "44971/44971 [==============================] - 3s 67us/step - loss: 0.3675 - val_loss: 0.2857\n",
      "Epoch 2/20\n",
      "44971/44971 [==============================] - 1s 16us/step - loss: 0.2819 - val_loss: 0.2738\n",
      "Epoch 3/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2582 - val_loss: 0.2498\n",
      "Epoch 4/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2399 - val_loss: 0.2313\n",
      "Epoch 5/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2282 - val_loss: 0.2258\n",
      "Epoch 6/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2238 - val_loss: 0.2224\n",
      "Epoch 7/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2196 - val_loss: 0.2175\n",
      "Epoch 8/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2154 - val_loss: 0.2146\n",
      "Epoch 9/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2128 - val_loss: 0.2124\n",
      "Epoch 10/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2103 - val_loss: 0.2099\n",
      "Epoch 11/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2080 - val_loss: 0.2079\n",
      "Epoch 12/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2060 - val_loss: 0.2059\n",
      "Epoch 13/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2043 - val_loss: 0.2046\n",
      "Epoch 14/20\n",
      "44971/44971 [==============================] - 1s 16us/step - loss: 0.2028 - val_loss: 0.2029\n",
      "Epoch 15/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2015 - val_loss: 0.2020\n",
      "Epoch 16/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.2003 - val_loss: 0.2006\n",
      "Epoch 17/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.1992 - val_loss: 0.1993\n",
      "Epoch 18/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.1981 - val_loss: 0.1985\n",
      "Epoch 19/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.1972 - val_loss: 0.1978\n",
      "Epoch 20/20\n",
      "44971/44971 [==============================] - 1s 15us/step - loss: 0.1964 - val_loss: 0.1970\n",
      "Train on 44971 samples, validate on 4997 samples\n",
      "Epoch 1/20\n",
      "44971/44971 [==============================] - 1s 24us/step - loss: 1.8934 - acc: 0.3156 - val_loss: 1.7898 - val_acc: 0.3276\n",
      "Epoch 2/20\n",
      "44971/44971 [==============================] - 0s 9us/step - loss: 1.7710 - acc: 0.3275 - val_loss: 1.7512 - val_acc: 0.3276\n",
      "Epoch 3/20\n",
      "44971/44971 [==============================] - 0s 9us/step - loss: 1.7220 - acc: 0.3390 - val_loss: 1.7077 - val_acc: 0.3620\n",
      "Epoch 4/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6900 - acc: 0.3771 - val_loss: 1.6896 - val_acc: 0.3850\n",
      "Epoch 5/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6785 - acc: 0.3856 - val_loss: 1.6814 - val_acc: 0.3868\n",
      "Epoch 6/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6706 - acc: 0.3852 - val_loss: 1.6745 - val_acc: 0.3876\n",
      "Epoch 7/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6610 - acc: 0.3842 - val_loss: 1.6588 - val_acc: 0.3796\n",
      "Epoch 8/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6497 - acc: 0.3808 - val_loss: 1.6447 - val_acc: 0.3784\n",
      "Epoch 9/20\n",
      "44971/44971 [==============================] - 0s 9us/step - loss: 1.6403 - acc: 0.3819 - val_loss: 1.6370 - val_acc: 0.3786\n",
      "Epoch 10/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6334 - acc: 0.3845 - val_loss: 1.6289 - val_acc: 0.3810\n",
      "Epoch 11/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6290 - acc: 0.3855 - val_loss: 1.6235 - val_acc: 0.3826\n",
      "Epoch 12/20\n",
      "44971/44971 [==============================] - 0s 9us/step - loss: 1.6256 - acc: 0.3880 - val_loss: 1.6210 - val_acc: 0.3860\n",
      "Epoch 13/20\n",
      "44971/44971 [==============================] - 0s 9us/step - loss: 1.6239 - acc: 0.3890 - val_loss: 1.6269 - val_acc: 0.3844\n",
      "Epoch 14/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6207 - acc: 0.3895 - val_loss: 1.6159 - val_acc: 0.3874\n",
      "Epoch 15/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6190 - acc: 0.3917 - val_loss: 1.6152 - val_acc: 0.3920\n",
      "Epoch 16/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6177 - acc: 0.3930 - val_loss: 1.6134 - val_acc: 0.3942\n",
      "Epoch 17/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6157 - acc: 0.3938 - val_loss: 1.6070 - val_acc: 0.3970\n",
      "Epoch 18/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6109 - acc: 0.3987 - val_loss: 1.6008 - val_acc: 0.4006\n",
      "Epoch 19/20\n",
      "44971/44971 [==============================] - 0s 8us/step - loss: 1.6072 - acc: 0.4000 - val_loss: 1.6043 - val_acc: 0.4066\n",
      "Epoch 20/20\n",
      "44971/44971 [==============================] - 0s 9us/step - loss: 1.6055 - acc: 0.4014 - val_loss: 1.5960 - val_acc: 0.4046\n",
      "8908/8908 [==============================] - 1s 87us/step\n",
      "Classification score for NN:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.31      0.16      0.21       925\n",
      "           2       0.30      0.05      0.09       975\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.40      0.02      0.04       199\n",
      "           5       0.00      0.00      0.00       828\n",
      "           6       0.27      0.53      0.36      1625\n",
      "           7       0.26      0.07      0.11      1247\n",
      "           8       0.53      0.84      0.65      2959\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      8908\n",
      "   macro avg       0.26      0.21      0.18      8908\n",
      "weighted avg       0.34      0.41      0.33      8908\n",
      "\n",
      "Classification score for SOAM1:               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.23      0.24      0.23       925\n",
      "           2       0.24      0.23      0.23       975\n",
      "           3       0.42      0.23      0.30       150\n",
      "           4       0.49      0.49      0.49       199\n",
      "           5       0.50      0.31      0.38       828\n",
      "           6       0.35      0.31      0.33      1625\n",
      "           7       0.33      0.21      0.26      1247\n",
      "           8       0.60      0.80      0.69      2959\n",
      "\n",
      "   micro avg       0.45      0.45      0.45      8908\n",
      "   macro avg       0.39      0.35      0.36      8908\n",
      "weighted avg       0.42      0.45      0.43      8908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#main code to run all functions to reach objective\n",
    "accuracy_list_NN,F1_score_list_NN,Precision_list_NN,Recall_list_NN,accuracy_list_SOAM1,F1_score_list_SOAM1,Precision_list_SOAM1,Recall_list_SOAM1=data_sampling(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33217332734620564,\n",
       " 0.3865065110013471,\n",
       " 0.3673102828917827,\n",
       " 0.37415806017063313,\n",
       " 0.39604849573417156,\n",
       " 0.4137853614728334,\n",
       " 0.4106421194431971,\n",
       " 0.4150202065559048,\n",
       " 0.4024472384373597,\n",
       " 0.41030534351145037]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evalution output for Neural network\n",
    "accuracy_list_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.41546924113156714,\n",
       " 0.43780871127076787,\n",
       " 0.4373596766951055,\n",
       " 0.43769645262685225,\n",
       " 0.4410642119443197,\n",
       " 0.4389312977099237,\n",
       " 0.43971710821733273,\n",
       " 0.4296138302649304,\n",
       " 0.45363718006286485,\n",
       " 0.44611585092052086]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evalution output for SOAM network\n",
    "accuracy_list_SOAM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('Accuracy_NN.txt', 'w') as f:\n",
    "    print(accuracy_list_NN, file=f)\n",
    "with open('Accuracy_SOAM.txt', 'w') as f:\n",
    "    print(accuracy_list_SOAM1, file=f)\n",
    "\n",
    "with open('F1_score_NN.txt', 'w') as f:\n",
    "    print(F1_score_list_NN, file=f)\n",
    "with open('F1_score_SOAM1.txt', 'w') as f:\n",
    "    print(F1_score_list_SOAM1, file=f)\n",
    "\n",
    "with open('Precision_Score_NN.txt', 'w') as f:\n",
    "    print(Precision_list_NN, file=f)\n",
    "with open('Precision_Score_SOAM1.txt', 'w') as f:\n",
    "    print(Precision_list_SOAM1, file=f)\n",
    "    \n",
    "with open('Recall_Recall_NN.txt', 'w') as f:\n",
    "    print(Recall_list_NN, file=f)\n",
    "with open('Recall_Recall_SOAM1.txt', 'w') as f:\n",
    "    print(Recall_list_SOAM1, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuaracy Value')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#output comparitive visualization\n",
    "from matplotlib.pyplot import figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(accuracy_list_NN,label='NN')\n",
    "plt.plot(accuracy_list_SOAM1,label='SOAM1')\n",
    "#plt.plot([10,20], accuracy_list_SOAM3,label='SOAM2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"data fraction\")\n",
    "plt.ylabel(\"Accuaracy Value\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
